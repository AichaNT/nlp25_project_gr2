{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eae3bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts.data_aug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m write_tsv_file, extract_labeled_tokens, mapping, read_tsv_file, write_iob2_file\n\u001b[1;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_aug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_aug_replace\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmiddle_eastern_ne\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_first_names, get_last_names,  load_location, load_organisation\n\u001b[1;32m     12\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scripts.data_aug'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.load_data import write_tsv_file, extract_labeled_tokens, mapping, read_tsv_file, write_iob2_file\n",
    "from scripts.data_aug import data_aug_replace\n",
    "from middle_eastern_ne import extract_first_names, get_last_names,  load_location, load_organisation\n",
    "\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdc040",
   "metadata": {},
   "source": [
    "## Get ME entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_BPER = extract_first_names(\"../data_aug_sources/Ordbog_over_muslimske_fornavne_i_DK.pdf\")\n",
    "ME_IPER = get_last_names(\"../data_aug_sources/middle_eastern_last_names.txt\", \"../data_aug_sources/KDBGIVE.tsv\")\n",
    "ME_LOC = load_location(\"../data_aug_sources/the-middle-east-cities.csv\")\n",
    "ME_ORG = load_organisation(\"../data_aug_sources/middle_eastern_organisations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d846a",
   "metadata": {},
   "source": [
    "## Read in data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_train = \"../data/no_overlap_da_news/da_news_train.tsv\"\n",
    "path_dev = \"../data/no_overlap_da_news/da_news_dev.tsv\"\n",
    "path_test = \"../data/no_overlap_da_news/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447948f",
   "metadata": {},
   "source": [
    "## Replace entities in dev and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac413feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all tokens in train data - to ensure no overlap later\n",
    "train_tokens = extract_labeled_tokens(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf776a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving all used entities\n",
    "used_entities = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29464575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount, ME_LOC = ME_LOC, ME_ORG = ME_ORG,\n",
    "                     ME_BPER = ME_BPER, ME_IPER = ME_IPER, used_entities = None, train_tokens=train_tokens):\n",
    "    \"\"\"\n",
    "    Replaces named entities in a subset of the dataset with new MENAPT ones, ensuring:\n",
    "    - No reused tokens across datasets\n",
    "    - No tokens from train set\n",
    "    - Deterministic behavior\n",
    "    - Returns updated used_entities (flat set of tokens)\n",
    "    \"\"\"\n",
    "    local_used = set(used_entities)\n",
    "\n",
    "    # extract sentences with containing relevant tags\n",
    "    eligible_sentences = [sent for sent in dataset if any(tag not in [\"O\", \"B-MISC\", \"I-MISC\"] for tag in sent[\"ner_tags\"])]\n",
    "    # select random sentences\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "    # create copy to not modify original dataset \n",
    "    modified_dataset = copy.deepcopy(dataset)\n",
    "\n",
    "    for sent in modified_dataset:\n",
    "        if sent not in selected_sentences:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                available = sorted([p for p in ME_BPER if p not in local_used and p not in train_tokens])\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    local_used.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                available = sorted([p for p in ME_IPER if p not in local_used and p not in train_tokens])\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    local_used.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    loc for loc in ME_LOC\n",
    "                    if len(loc[\"tokens\"]) == span_len and\n",
    "                    tuple(loc[\"tokens\"]) not in local_used and\n",
    "                    tuple(loc[\"tokens\"]) not in train_tokens\n",
    "                ]\n",
    "                \n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    local_used.add(tuple(replace[\"tokens\"]))\n",
    "\n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    org for org in ME_ORG\n",
    "                    if len(org[\"tokens\"]) == span_len and\n",
    "                    tuple(org[\"tokens\"]) not in local_used and\n",
    "                    tuple(org[\"tokens\"]) not in train_tokens\n",
    "                ]\n",
    "\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    local_used.add(tuple(replace[\"tokens\"]))\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return modified_dataset, local_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_dev, used_entities = data_aug_replace(dev_data, sentence_amount=1000,\n",
    "                                         ME_LOC = ME_LOC, ME_ORG = ME_ORG, ME_BPER = ME_BPER, ME_IPER = ME_IPER, \n",
    "                                         used_entities = used_entities, train_tokens=train_tokens)\n",
    "len(used_entities)\n",
    "\n",
    "ME_test, used_entities = data_aug_replace(test_data, sentence_amount=1000,\n",
    "                                         ME_LOC = ME_LOC, ME_ORG = ME_ORG, ME_BPER = ME_BPER, ME_IPER = ME_IPER, \n",
    "                                         used_entities = used_entities, train_tokens=train_tokens)\n",
    "len(used_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(used_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ME_BPER))\n",
    "BPER_left = [item for item in ME_BPER if item not in used_entities]\n",
    "print(len(BPER_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd66e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ME_IPER))\n",
    "IPER_left = [item for item in ME_IPER if item not in used_entities]\n",
    "print(len(IPER_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ME_LOC))\n",
    "LOC_left = [d for d in ME_LOC if tuple(d['tokens']) not in used_entities]\n",
    "print(len(LOC_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb33ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ME_ORG))\n",
    "ORG_left = [d for d in ME_ORG if tuple(d['tokens']) not in used_entities]\n",
    "print(len(ORG_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ad0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sent in ME_dev: \n",
    "#    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sent in ME_test: \n",
    "#    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a594f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as tsv files\n",
    "write_tsv_file(ME_dev, \"../data/me_data/middle_eastern_dev.tsv\")\n",
    "write_tsv_file(ME_test, \"../data/me_data/middle_eastern_test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287171b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_iob2_file(ME_test, path=\"../data/me_data/middle_eastern_test.iob2\", gold=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
