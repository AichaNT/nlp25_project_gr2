{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0eae3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.load_data import write_tsv_file, extract_labeled_tokens, mapping, read_tsv_file, write_iob2_file\n",
    "from middle_eastern_ne import extract_first_names, get_last_names,  load_location, load_organisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdc040",
   "metadata": {},
   "source": [
    "## Get ME entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6bf85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_BPER = extract_first_names(\"../data_aug_sources/Ordbog_over_muslimske_fornavne_i_DK.pdf\")\n",
    "ME_IPER = get_last_names(\"../data_aug_sources/middle_eastern_last_names.txt\", \"../data_aug_sources/KDBGIVE.tsv\")\n",
    "ME_LOC = load_location(\"../data_aug_sources/the-middle-east-cities.csv\")\n",
    "ME_ORG = load_organisation(\"../data_aug_sources/middle_eastern_organisations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d14fffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Saudi', 'Aramco'], 'ner_tags': ['B-ORG', 'I-ORG']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ME_ORG[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d846a",
   "metadata": {},
   "source": [
    "## Read in data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f38682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_train = \"../data/no_overlap_da_news/da_news_train.tsv\"\n",
    "path_dev = \"../data/no_overlap_da_news/da_news_dev.tsv\"\n",
    "path_test = \"../data/no_overlap_da_news/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447948f",
   "metadata": {},
   "source": [
    "## Replace entities in dev and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac413feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all tokens in train data - to ensure no overlap later\n",
    "train_tokens = extract_labeled_tokens(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbf776a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving all used entities\n",
    "used_entities = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26cec93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount, ME_LOC = ME_LOC, ME_ORG = ME_ORG,\n",
    "                     ME_BPER = ME_BPER, ME_IPER = ME_IPER, used_entities = used_entities, train_tokens=train_tokens):\n",
    "    \"\"\"\n",
    "    Replaces named entities in a subset of the dataset with new MENAPT ones, ensuring no reuse across datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(42)\n",
    "\n",
    "    # extract sentences with containing relevant tags\n",
    "    eligible_sentences = [sent for sent in dataset if any(tag not in [\"O\", \"B-MISC\", \"I-MISC\"] for tag in sent[\"ner_tags\"])]\n",
    "    # select random sentences\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "    # create copy to not modify original dataset \n",
    "    modified_dataset = [dict(sent) for sent in dataset] \n",
    "\n",
    "    for sent in modified_dataset:\n",
    "        if sent not in selected_sentences:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                available = [p for p in ME_BPER if p not in used_entities and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    used_entities.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                available = [p for p in ME_IPER if p not in used_entities and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    used_entities.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "\n",
    "                i += 1\n",
    "\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    loc for loc in ME_LOC\n",
    "                    if not any(token in train_tokens for token in loc[\"tokens\"])\n",
    "                    and not any(token in used_entities for token in loc[\"tokens\"])\n",
    "                    and len(loc[\"tokens\"]) == span_len\n",
    "                ]\n",
    "                \n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    used_entities.update(replace[\"tokens\"])\n",
    "\n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    org for org in ME_ORG\n",
    "                    if not any(token in train_tokens for token in org[\"tokens\"])\n",
    "                    and not any(token in used_entities for token in org[\"tokens\"])\n",
    "                    and len(org[\"tokens\"]) == span_len\n",
    "                ]\n",
    "\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    used_entities.update(replace[\"tokens\"])\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return modified_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c07f7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount, ME_LOC = ME_LOC, ME_ORG = ME_ORG,\n",
    "                     ME_BPER = ME_BPER, ME_IPER = ME_IPER, used_entities = used_entities, train_tokens=train_tokens):\n",
    "    \"\"\"\n",
    "    Replaces named entities in a subset of the dataset with new MENAPT ones, ensuring:\n",
    "    - No reused tokens across datasets\n",
    "    - No tokens from train set\n",
    "    - Deterministic behavior\n",
    "    - Returns updated used_entities (flat set of tokens)\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    local_used = set(used_entities)\n",
    "    modified_dataset = [dict(sent) for sent in dataset]\n",
    "\n",
    "    eligible_sentences = [\n",
    "        sent for sent in modified_dataset\n",
    "        if any(tag not in [\"O\", \"B-MISC\", \"I-MISC\"] for tag in sent[\"ner_tags\"])\n",
    "    ]\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "\n",
    "    for sent in modified_dataset:\n",
    "        if sent not in selected_sentences:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                available = [p for p in ME_BPER if p not in local_used and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    local_used.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                available = [p for p in ME_IPER if p not in local_used and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    local_used.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    loc for loc in ME_LOC\n",
    "                    if len(loc[\"tokens\"]) == span_len and\n",
    "                    all(tok not in train_tokens and tok not in local_used for tok in loc[\"tokens\"])\n",
    "                ]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    local_used.update(replace[\"tokens\"])\n",
    "\n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    org for org in ME_ORG\n",
    "                    if len(org[\"tokens\"]) == span_len and\n",
    "                    all(tok not in train_tokens and tok not in local_used for tok in org[\"tokens\"])\n",
    "                ]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    local_used.update(replace[\"tokens\"])\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return modified_dataset, local_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6025d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_dev, used_entities = data_aug_replace(dev_data, 1000)\n",
    "ME_test, used_entities = data_aug_replace(test_data, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75b7f975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Kurdi', 'Alshaya', 'Sarah', 'Saliha', 'Arryadia', 'EgyptAir', 'Yozgat', 'Cevital', 'Kandil', 'Al badour', 'ONTV', 'Dnata', 'Emsteel', 'Ilyas', 'G42', 'Wikaya', 'e&', 'Gorgan', 'Siddiqui', 'Sahir', 'Morad', 'Randa', 'Talha', 'SNRT', 'Omer', 'Epilert', 'Zaxo', 'Al-Anbaa', 'Stock', 'Faleh', 'Haider', 'Samed', 'Afriquia', 'ZenHR', 'Isparta', 'Dabchy', 'Wail', 'Ad', 'DenizBank', 'Baneh', 'Burson', 'Kadir', 'Fouad', 'Wasim', 'Marivan', 'Mobily', 'Farhad', 'Shahr', 'Khalif', 'Zakaria', 'al-Jadid', 'Bahram', 'Arabot', 'Bukhari', 'Abdul', 'Habbouch', 'Ridha', 'Trabzon', 'Mardin', 'Kashmar', 'Yaser', 'Sehm', 'Starworld', 'Umm', 'Agri-Nutrients', 'Muhsin', 'Krishan', 'Monoprix', 'Fatsa', 'Partners', 'Healthcare', 'Anika', 'Sherhan', 'Shamsina', 'Assabeel', 'Benyamin', 'Omdurman', 'Piranshahr', 'Malik', 'ACWA', 'Bin Chemlane', 'Imane', 'Al-Waie', 'Ümraniye', 'Sari', 'Ordu', 'Echorouk', 'Uşak', 'Amin', 'Tabbah', 'Wael', 'Ajeer', 'Hanadi', 'Marsteller', 'Khatabh', 'Party', 'Saudi', 'Navid', 'Mohiuddin', 'Yasemin', 'Ereğli', 'Eskisehir', 'Naia', 'Tahira', 'Isna', 'Sabah', '360VUZ', 'Baki', 'Umair', 'Nadja', 'May', 'Rahlah', 'Raghavan', 'Rize', 'Omantel', 'Almutairi', 'Khalaf', 'Said', 'Damir', 'Qasr', 'Alaitaysi', 'Tariq', 'Al', 'Saham', 'Maher', 'Mustapha', 'Aster', 'alrajhi', 'Matruh', 'al-Hayat', 'Ikram', 'Al-Ahram', 'Ismail', 'Kudu', 'Khanjarah', 'Sahmu Edin', 'Mo’men', 'Bünyamin', 'Esenler', 'Zahraa', \"Ha'il\", 'Abosaymah', 'Nasreen', 'MBC', 'Lana', 'Othman', 'Sahar', 'Jamila', 'Properties', 'Latif', 'Shams', 'Majid', 'ADNOC', 'Egyptalum', 'News', 'Kerman', 'Alissa', 'Egypten', 'Bam', 'PubliTools', 'Mounira', 'Medina', 'GoEjaza', 'Ibb', 'Zaina', 'Feras', 'Aida', 'Majida', 'Denizli', 'Furkan', 'Kamila', 'Walid', 'Noor', 'Babol', 'Manna', 'Warda', 'Sharjah', 'Emaar', 'Tamer', 'Awwad', 'Siera', 'Elm', 'Zarifa', 'El', 'Nasr', 'Ghada', 'Ameerh', 'Safa', 'Amer', 'Mohammed', 'Zaineb', 'Samer', 'Daud', 'Unifonic', 'Yousef', 'Faris', 'Çorum', 'Torbat-e', 'Tripoli', 'al-Muslimah', 'Zeina', 'Ramadi', 'Cinescape', 'Altiwaytin', 'Dalila', 'Ar', 'Gas', 'Diyarbakir', 'Careem', 'PSLab', 'Tarut', 'Liban', 'Daily', 'Munira', 'Najma', 'Aliya', 'Sajjad', 'al-Watan', 'al-Zahf', 'Alairaymin', 'Jonubi', 'Hawa', 'Abdalla', 'Education', 'Rifa', 'Arasco', 'Ul’Jadid', 'Nidal', 'Manal', 'Comarit', 'Al masaru', 'Al naimi', 'AKKASA', 'Khawaldeh', 'Nablus', 'Suez', 'Al-Hudaydah', 'Nazarabad', 'Yamina', 'Libyana', 'Aydin', 'Shahin', 'Ola', 'Restaurants', 'Al zawaydah', 'Nazmi', 'Abdul jabbar', 'Mehwar', 'Arwa', 'Abdullahi', 'Sofia', 'Shadi', 'Sehal', 'Zahida', 'Al-Ahd', 'Nariman', 'SPORTS', 'Allawi', 'Al-Yawm', 'Communications', 'al-Jarida', 'al-Arabi', 'Abed', 'SABIC', 'Muhamed', 'Shah', 'Aziz', 'Alaijayzi', 'Salamah', 'Aaytoday', 'Muhammad', 'Bandırma', 'Tahani', 'Exchange', 'Kermanshah', 'Aleppo', 'Iğdır', 'Karabük', 'Mervan', 'Mahnaz', 'Albyaty', 'Samawah', 'Rehana', 'Naftal', 'Somaca', 'Istiqlal', 'Nehmeh', 'Ahsan', 'Satellite', 'Fetchr', 'Salih', 'Aladdin', 'Afane', 'Fasa', 'Sawalimah', 'Port', 'Hawacom', 'Janna', 'Asiya', 'Darab', 'Hanif', 'Yah', 'Solutions', 'Dussur', 'Matin', 'Worldwide', 'Al zawahiri', 'Bin', 'Talat', 'Shorooq', 'Marafiq', 'Aswan', 'Khalid', 'Houda', 'Development', 'Sawamira', 'Kaka', 'Hajar', 'Aftab', 'Mirza', 'Al-Ouruba', 'Nuran', 'Azhar', 'beIN', 'KTV2', 'Hazem', 'Alaithaymi', 'Electroplanet', 'Samir', 'Gulf', 'Nouakchott', 'Yonis', 'Dubai', 'Mazhar', 'Kamran', 'LDC', 'Bonab', 'Cima', 'Faik', 'Najla', 'Wahida', 'As', 'Malayer', 'Qaym', 'SOMED', 'Jasmine', 'al-Maghribia', 'Aldhimayri', 'Aiman', 'Mubadala', 'Télé', 'Madani', 'al-Jadida', 'Khazal', 'Matar', 'Dahuk', 'Hamdi', \"D'souza\", 'Lamsa', 'Sabit', 'Rachid', 'Relief', 'Zamzam', 'Djezzy', 'Hassan', 'Khoy', 'Sania', 'al-Fajr', 'Airline', 'Viranşehir', 'Gamal', 'Nizip', 'Power', 'Managem', 'Ubah', 'Nabil', 'Umar', 'Asdaa', 'Abjjad', 'Kilis', 'Company', 'Ishaq', 'Basheer', 'Nagham', 'Mrsool', 'Jumeirah', 'Abir', 'Dah', 'Al-Arab', 'Mahmud', 'Muscat', 'Damac', 'Adaraweesh', 'Ahar', 'Telecom', 'Solfeh', 'Abdullah', 'Almarai', 'Khomeyni', 'Tasnim', 'Karima', 'Samsun', 'Ezzat', 'Souqalmal', 'Albawainah', 'Americana', 'Khash', 'Çankaya', 'Samia', 'Abha', 'Zaher', 'Saudia', 'Al-Akhdar', 'Najat', 'Anzal-e', 'Abid', 'Lim', 'Altinayzi', 'Emirates', 'Shahida', 'Borouge', 'Iqra', 'Soma', 'Ziad', 'Muratpasa', 'Aumet', 'Laila', 'Sanaa', 'Habib', 'Aldar', 'Burdur', 'Suhair', 'Safiya', 'Diwaniyah', 'Elbeltedji', 'bank', 'Ayoub', 'Beirut', 'Sawani', 'Kavus', 'Salalah', 'Langarud', 'Sur', 'Dhamar', 'Suresh', 'Ibrahim', 'Nahavand', 'Widad', 'Islamic', 'Ihab', 'DMC', 'Tarek', 'Filasteen', 'Kesmoulkerim', 'TGCC', 'Iqraa', 'Latifa', 'Zakia', 'Gonbad-e', 'Jalal', 'Sohela', 'Nahid', 'Wallyscar', 'Hebron', 'El maghraby', 'Naseej', 'Maktoof', 'Mersa', 'Sabiha', 'SGTM', 'Investments', 'Sakina', 'Hanaa', 'Attajdid', 'Heydariyeh', 'Ben', 'Maroc', 'Aya', 'Swvl', 'Ala', 'Adhriy', 'Ramin', 'Sioara', 'Shoaib', 'Salim', 'DM', 'Çubuk', 'Dalia', 'Yallacompare', 'Arabien', 'Al habsi', 'Bahçelievler', 'Tahir', 'Tanda', 'GEMS', 'Al-Kifah', 'Nazilli', 'Al issa', 'Kuşadası', 'Bafra', 'Mauritel', 'Manbij', 'Qarchak', 'Hotels', 'Akhbar'}\n"
     ]
    }
   ],
   "source": [
    "print(used_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce5ad0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sent in ME_dev: \n",
    "#    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f48b4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sent in ME_test: \n",
    "#    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81a594f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as tsv files\n",
    "write_tsv_file(ME_dev, \"../data/me_data/middle_eastern_dev.tsv\")\n",
    "write_tsv_file(ME_test, \"../data/me_data/middle_eastern_test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "287171b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_iob2_file(ME_test, path=\"../data/me_data/middle_eastern_test.iob2\", gold=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
