{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "d0eae3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.load_data import write_tsv_file, extract_labeled_tokens, mapping, read_tsv_file, write_iob2_file\n",
    "from middle_eastern_ne import extract_first_names, get_last_names,  load_location, load_organisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdc040",
   "metadata": {},
   "source": [
    "## Get ME entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "c6bf85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_BPER = extract_first_names(\"../data_aug_sources/Ordbog_over_muslimske_fornavne_i_DK.pdf\")\n",
    "ME_IPER = get_last_names(\"../data_aug_sources/middle_eastern_last_names.txt\", \"../data_aug_sources/KDBGIVE.tsv\")\n",
    "ME_LOC = load_location(\"../data_aug_sources/the-middle-east-cities.csv\")\n",
    "ME_ORG = load_organisation(\"../data_aug_sources/middle_eastern_organisations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "8d14fffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Saudi', 'Aramco'], 'ner_tags': ['B-ORG', 'I-ORG']}"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ME_ORG[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d846a",
   "metadata": {},
   "source": [
    "## Read in data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "2f38682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_train = \"../data/no_overlap_da_news/da_news_train.tsv\"\n",
    "path_dev = \"../data/no_overlap_da_news/da_news_dev.tsv\"\n",
    "path_test = \"../data/no_overlap_da_news/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447948f",
   "metadata": {},
   "source": [
    "## Replace entities in dev and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "ac413feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all tokens in train data - to ensure no overlap later\n",
    "train_tokens = extract_labeled_tokens(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "fbf776a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving all used entities\n",
    "used_entities = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "26cec93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount, ME_LOC = ME_LOC, ME_ORG = ME_ORG,\n",
    "                     ME_BPER = ME_BPER, ME_IPER = ME_IPER, used_entities = used_entities, train_tokens=train_tokens):\n",
    "    \"\"\"\n",
    "    Replaces named entities in a subset of the dataset with new MENAPT ones, ensuring no reuse across datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(42)\n",
    "\n",
    "    # extract sentences with containing relevant tags\n",
    "    eligible_sentences = [sent for sent in dataset if any(tag not in [\"O\", \"B-MISC\", \"I-MISC\"] for tag in sent[\"ner_tags\"])]\n",
    "    # select random sentences\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "    # create copy to not modify original dataset \n",
    "    modified_dataset = [dict(sent) for sent in dataset] \n",
    "\n",
    "    for sent in modified_dataset:\n",
    "        if sent not in selected_sentences:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                available = [p for p in ME_BPER if p not in used_entities and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    used_entities.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                available = [p for p in ME_IPER if p not in used_entities and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    used_entities.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "\n",
    "                i += 1\n",
    "\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    loc for loc in ME_LOC\n",
    "                    if not any(token in train_tokens for token in loc[\"tokens\"])\n",
    "                    and not any(token in used_entities for token in loc[\"tokens\"])\n",
    "                    and len(loc[\"tokens\"]) == span_len\n",
    "                ]\n",
    "                \n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    used_entities.update(replace[\"tokens\"])\n",
    "\n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    org for org in ME_ORG\n",
    "                    if not any(token in train_tokens for token in org[\"tokens\"])\n",
    "                    and not any(token in used_entities for token in org[\"tokens\"])\n",
    "                    and len(org[\"tokens\"]) == span_len\n",
    "                ]\n",
    "\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    used_entities.update(replace[\"tokens\"])\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return modified_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "c07f7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount, ME_LOC = ME_LOC, ME_ORG = ME_ORG,\n",
    "                     ME_BPER = ME_BPER, ME_IPER = ME_IPER, used_entities = None, train_tokens=train_tokens):\n",
    "    \"\"\"\n",
    "    Replaces named entities in a subset of the dataset with new MENAPT ones, ensuring:\n",
    "    - No reused tokens across datasets\n",
    "    - No tokens from train set\n",
    "    - Deterministic behavior\n",
    "    - Returns updated used_entities (flat set of tokens)\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    local_used = set(used_entities)\n",
    "    modified_dataset = [dict(sent) for sent in dataset]\n",
    "\n",
    "    eligible_sentences = [\n",
    "        sent for sent in modified_dataset\n",
    "        if any(tag not in [\"O\", \"B-MISC\", \"I-MISC\"] for tag in sent[\"ner_tags\"])\n",
    "    ]\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "\n",
    "    for sent in modified_dataset:\n",
    "        if sent not in selected_sentences:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                available = [p for p in ME_BPER if p not in local_used and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    local_used.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                available = [p for p in ME_IPER if p not in local_used and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    local_used.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    loc for loc in ME_LOC\n",
    "                    if len(loc[\"tokens\"]) == span_len and\n",
    "                    all(tok not in train_tokens and tok not in local_used for tok in loc[\"tokens\"])\n",
    "                ]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    local_used.update(replace[\"tokens\"])\n",
    "\n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    org for org in ME_ORG\n",
    "                    if len(org[\"tokens\"]) == span_len and\n",
    "                    all(tok not in train_tokens and tok not in local_used for tok in org[\"tokens\"])\n",
    "                ]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    local_used.update(replace[\"tokens\"])\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return modified_dataset, local_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "29464575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount, ME_LOC = ME_LOC, ME_ORG = ME_ORG,\n",
    "                     ME_BPER = ME_BPER, ME_IPER = ME_IPER, used_entities = None, train_tokens=train_tokens):\n",
    "    \"\"\"\n",
    "    Replaces named entities in a subset of the dataset with new MENAPT ones, ensuring:\n",
    "    - No reused tokens across datasets\n",
    "    - No tokens from train set\n",
    "    - Deterministic behavior\n",
    "    - Returns updated used_entities (flat set of tokens)\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    local_used = set(used_entities)\n",
    "    modified_dataset = [dict(sent) for sent in dataset]\n",
    "\n",
    "    eligible_sentences = [\n",
    "        sent for sent in modified_dataset\n",
    "        if any(tag not in [\"O\", \"B-MISC\", \"I-MISC\"] for tag in sent[\"ner_tags\"])\n",
    "    ]\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "\n",
    "    for sent in modified_dataset:\n",
    "        if sent not in selected_sentences:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                available = [p for p in ME_BPER if p not in local_used and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    local_used.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                available = [p for p in ME_IPER if p not in local_used and p not in train_tokens]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][i] = replace\n",
    "                    local_used.add(replace)\n",
    "                i += 1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    loc for loc in ME_LOC\n",
    "                    if len(loc[\"tokens\"]) == span_len and\n",
    "                    all(tok not in train_tokens and tok not in local_used for tok in loc[\"tokens\"])\n",
    "                ]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    local_used.add(tuple(replace[\"tokens\"]))\n",
    "\n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                available = [\n",
    "                    org for org in ME_ORG\n",
    "                    if len(org[\"tokens\"]) == span_len and\n",
    "                    all(tok not in train_tokens and tok not in local_used for tok in org[\"tokens\"])\n",
    "                ]\n",
    "                if available:\n",
    "                    replace = random.choice(available)\n",
    "                    sent[\"tokens\"][span_start:span_start + span_len] = replace[\"tokens\"]\n",
    "                    local_used.add(tuple(replace[\"tokens\"]))\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return modified_dataset, local_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "0473ac72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "6025d8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ME_dev, used_entities = data_aug_replace(dev_data, used_entities=used_entities, sentence_amount=1000)\n",
    "len(used_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "fe68fcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ME_test, used_entities = data_aug_replace(test_data, used_entities=used_entities, sentence_amount=1000)\n",
    "len(used_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "75b7f975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Zeytinburnu',), ('Wallyscar',), ('Aramco',), ('Deir', 'ez-Zor'), ('Al', 'Jubayl'), 'Bin', ('Ashmun',), 'Hamoud', ('Spinneys',), 'Bushra', ('Munuf',), 'Jalal', ('Patnos',), ('Sulaymaniyah',), ('Housh', 'Eissa'), 'Sabri', ('OSN',), 'El kutby', 'Kerem', 'Farhan', 'Saber', 'Sawahah', 'Roya', ('Ödemiş',), 'Mawmad', ('Abadan',), 'Sabiha', ('Aden',), 'Muhamed', 'Jasmin', 'Haydar', 'Alyami', 'Yousif', 'Arzu', ('Al-Thawra',), 'Alaidhayqi', 'Sudany', 'Anas', 'Helmy', ('Somaca',), ('Kars',), 'Nazli', ('Gemlik',), ('Biougnach',), 'Asaad', 'Kazem', ('Yozgat',), ('Ahlibank',), 'Laila', 'Asma', 'Zarifa', 'Issam', 'Fadila', ('Al', 'Sharq'), ('Emaar', 'Properties'), ('Banque', 'Saudi', 'Fransi'), 'Kadir', ('Koutoubia',), 'Naveed', ('Aswan',), 'Samia', ('Ismailia',), 'Yusuf', 'Zaina', ('Izmit',), ('Edirne',), ('Antalya',), ('Dubai', 'Investments'), 'Muhamad', 'Nadiya', ('Ramadi',), ('Solfeh',), ('al-Balad',), ('Argaam',), 'Fuqaha', 'Sheikh', 'Yusra', 'Ayah', 'Meriam', 'Mouna', 'Muhsin', ('Sorgun',), ('Tokat',), ('Al-Tijari',), 'Ghani', 'Matin', 'Recep', ('Attijariwafa', 'bank', 'group'), ('Al', 'Raya'), 'Hussein', 'Sudaa', ('Bushehr',), 'Iqra', 'Mourad', 'Salman', ('Nizip',), 'Benyamin', 'Nuri', 'Al aqrabawi', 'Maysa', 'Nour', 'Albahari', 'Fahad', ('BulkWhiz',), ('Dammam',), 'Daud', ('AKKASA',), ('ViaVii',), 'Javed', ('JoSat',), 'Raad', ('Amol',), 'Husam', ('Konya',), ('As', 'Safirah'), ('DenizBank',), 'Haider', ('Ajlun',), ('Khorramshahr',), ('Şişli',), 'Basil', 'Asra', ('Isfahan',), ('BMMI',), 'Zafar', 'Jalil', 'Abdel rahman', ('Al-Horria',), 'Albostaji', 'Mustapha', 'Talat', 'Amaal', 'Mohamed', ('Tunisavia',), ('Wanasah',), 'Hamad', 'Khan', ('Al', 'Eqtisadiah'), 'Al khaili', ('Empower',), ('Daraty',), 'Sa', ('Sonasid',), 'Noor', 'Sajjad', 'Sahar', ('Rize',), 'Murad', 'Sawakhina', ('Karabağlar',), 'Kessimou Elfedil', ('Libanon',), 'Riad', 'Fatuma', 'Ismael', 'Alairayshiah', 'Rashida', ('Khomeyni', 'Shahr'), ('MarsaMaroc',), ('Mashreq',), 'Saman', ('Arryadia',), 'Samir', 'Sadia', ('Al-Hasakah',), 'Saliha', 'Sameer', 'Dalal', (\"Ha'il\",), ('Ad', 'Diwaniyah'), 'Sawamira', ('Laraki',), ('HalalaH',), 'Abdullahi', ('Jordan',), ('Islamic', 'Relief', 'Worldwide'), ('3ayez',), ('Al', 'Matariyah'), ('El', 'Hiwar', 'El', 'Tounousi'), ('Al', 'Khankah'), 'Yunus', 'Alaisayti', ('Asmidal',), 'Sirin', ('Emirates', 'NBD'), ('Asyut',), ('Bupa', 'Arabia'), 'Talal', 'Adam', ('Riyadh',), 'Azad', ('Tunisna',), ('RASCO',), 'Zuhra', 'Hafsa', 'Najma', 'Kamran', 'Sakina', (\"Ta'if\",), 'Korkis', 'Gamal', 'Farah', 'Al zariqat', 'Hinna', 'Zakia', ('Qom',), ('Lucidya',), 'Afrah', ('Americana', 'Restaurants'), 'Obeidat', 'Waqas', 'Rahlah', 'Hashim', 'Selim', 'Radia', ('Gebze',), 'Hoda', 'Suleiman', ('Yallacompare',), ('Emaar', 'Development'), 'Bader', 'Khadra', ('Nusaybin',), 'Alaisaymi', ('ibTECHar', 'Digital', 'Solutions'), 'Elaiouaini', ('Faqus',), 'Sahmu Adin', 'Nazih’', ('alrajhi', 'bank'), 'Saleem', ('Al', 'Anbat'), 'Elgibaichi', 'Safiya', ('Shamsina',), ('Amran',), 'Rima', ('Kadirli',), 'Baki', 'Majid', 'Shahin', ('Mrsool',), 'Kanaan', 'Sonya', 'Maliha', 'Faysal', ('Al-Muhaidib',), ('Mathaqi',), 'Naila', ('DEWA',), ('Emirates', 'Islamic'), ('Asdaa', 'Burson', 'Marsteller'), ('Al', 'Watan'), ('Bojnurd',), ('Talabat',), ('Adwya',), ('Yasuj',), 'Alharbi', 'Lamia', 'Nariman', 'Heba', ('Qalyub',), ('Kozan',), ('Al', 'Aoula'), ('LDC',), ('Quchan',), 'Abed', 'Zaid', ('Azadshahr',), 'Dahlia', 'Zahraa', ('Uşak',), ('Electroplanet',), ('Sakakah',), ('Swvl',), 'Hasan', 'Al falahi', 'Mohammad', 'Nazif', ('Zarqa',), 'Zaineb', 'Nahla', ('Piranshahr',), ('Samalut',), 'Tarash', ('Dussur',), 'Haifa', ('As-Salt',), 'Brahim', 'Ashraf', ('Ash', 'Shafa'), 'Amal', 'Basha', ('Bagdad',), ('Wasla',), 'Kashif', 'Bayan', ('Zanjan',), 'Ihsan', 'Nidal', ('Jamalon',), ('al-Watan',), 'Ahmet', 'Nihal', 'Al khanbouli', ('Kahramanmaraş',), ('Jablah',), ('Erciş',), 'Elgidaibi', 'Sumaya', 'Intisar', ('Gulf', 'News'), 'Aisha', 'Hamoury', 'Mahdi', 'Fatma', ('Noon',), ('Sayhat',), ('SNVI',), 'Ubah', ('Aksaray',), ('Dabchy',), 'Nadir', 'Malaika', 'Nasreen', 'Mostafa', 'Kamil', 'Safa', ('Kilis',), ('NBN',), ('Taiz',), ('Al', 'Rai'), 'Alairayai', 'Jeddah', ('Esenler',), 'Salwa', 'Alaishayri', ('Cerebras',), 'Shazia', 'Nazim', ('Ad', 'Diyar'), 'Dakhil', 'Sahu', ('Gulfsat',), 'Nada', ('Malaeb',), 'Abdulahi', 'Abdulqadir', 'Tahira', 'Hamdi', 'Sulayman', 'Abdulaziz', 'Yousef', ('Çankaya',), 'Aldhimayni', 'Aliya', ('Egypten',), 'Duaa', 'Nisa', ('Tarut',), ('Al', 'Ain'), ('Iqraa',), 'Bakhit', ('Kezad',), ('Dnata',), 'Ayuub', 'Alaidhaymi', 'Zaki', 'Azzam', 'El maghraby', 'Milad', ('QEWC',), 'Leyla', ('Østjerusalem',), 'Morsy', ('Jirja',), 'Das', 'Sawamirah', ('AL24', 'News'), 'Nayef’', ('Zefta',), 'Safia', ('e&',), 'Wafa', 'Khatabh', ('SGTM',), 'Sahlan', ('Echorouk',), 'Nabil', ('Tabriz',), 'Al shuraideh', 'Jasmine', 'Mohsen', 'Elbandari', ('DMC',), ('Şanlıurfa',), ('Alwasat',), ('Varamin',), ('Jamjamal',), 'Mansoor', 'Lana', ('Epilert',), 'Abdalla', ('Lüleburgaz',), 'Al zaabi', 'Al zuben', 'Inas', ('Kastamonu',), ('Langarud',), 'Nadia', ('KIPCO',), ('Ma’aden',), ('Khanjarah',), ('Nilesat',), 'Rasheed', ('Takestan',), 'Hashem', 'Miriam', 'Hanaa', 'Rani', ('Alvand',), ('ZenHR', 'Solutions'), 'Mezal', 'Albanbali', ('Denizli',), 'Khalida', ('Esenyurt',), 'Rabih', 'Mansour', ('Neyshabur',), 'Azra', ('Torbat-e', 'Heydariyeh'), 'Malkawi', ('Naseej',), ('Saidal',), 'Mina', 'Israa', 'Mughal', 'Ajmal', ('Al', 'Massar'), ('Chefaa',), 'Ghulam', ('Massaya',), 'Farid', 'Harun', ('Saudia',), ('Tabbah',), 'Maha', 'Sahlaah', ('Naftal',), 'Nasir', 'Mervan', ('Bishah',), ('Palæstina',), 'Jamshid', ('Altibbi',), 'Fayez', ('Télé', 'Liban'), 'Rifat', ('Akhbar', 'Nouakchott'), ('Aleppo',), 'Emad', ('Ferrimaroc',), 'Hazim', 'Saja', 'Yonas', ('Fertiglobe',), ('Khash',), ('Evertek',), 'Bilal', ('Zahle',), ('Tabuk',), 'Altihayshi', ('Shorooq', 'Partners'), ('Khamis', 'Mushait'), 'Ghassan', 'Saif', ('Abha',), 'Fuad', ('Amanat', 'Holdings'), ('Al', 'Amal'), 'Marza', 'Yusef', 'Karam', ('PSLab',), ('Ordu',), 'Wafaa', 'Sabit', ('Baladna',), ('Al-Wehda',), 'Omer', 'Sawaghinah', ('Al-Mustaqbal',), ('Nadec',), ('Kahriz',), 'Albabun', 'May', ('Kermanshah',), 'Gani', 'Ghanem', 'Habiba', ('Al', 'Mada'), 'Aya', ('al-Qabas',), 'Kesmoulkerim', 'Sioar', ('Syriatel',), ('Filasteen', 'al-Muslimah'), 'Tasnim', ('Silvan',), ('Damanhur',), 'Walid', ('Torath',), 'Ehsan', 'Hammadi', ('Balad', 'Party'), 'Mehdi', 'Muhammed', ('Samarra',), 'Aicha', ('Körfez',), 'Khaled', 'Jaber', 'Tariq', ('MBC',), 'Karima', ('Gaza',), ('KTV2',), 'Syed', ('Al', 'Fujayrah'), 'Dunia'}\n"
     ]
    }
   ],
   "source": [
    "print(used_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "a66e33c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735\n",
      "517\n"
     ]
    }
   ],
   "source": [
    "print(len(ME_BPER))\n",
    "BPER_left = [item for item in ME_BPER if item not in used_entities]\n",
    "print(len(BPER_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "9fd66e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580\n",
      "1449\n"
     ]
    }
   ],
   "source": [
    "print(len(ME_IPER))\n",
    "IPER_left = [item for item in ME_IPER if item not in used_entities]\n",
    "print(len(IPER_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "c5ef21b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n",
      "369\n"
     ]
    }
   ],
   "source": [
    "print(len(ME_LOC))\n",
    "LOC_left = [d for d in ME_LOC if tuple(d['tokens']) not in used_entities]\n",
    "print(len(LOC_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "7eb33ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n",
      "302\n"
     ]
    }
   ],
   "source": [
    "print(len(ME_ORG))\n",
    "ORG_left = [d for d in ME_ORG if tuple(d['tokens']) not in used_entities]\n",
    "print(len(ORG_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "ce5ad0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sent in ME_dev: \n",
    "#    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "f48b4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sent in ME_test: \n",
    "#    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "81a594f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as tsv files\n",
    "write_tsv_file(ME_dev, \"../data/me_data/middle_eastern_dev.tsv\")\n",
    "write_tsv_file(ME_test, \"../data/me_data/middle_eastern_test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "287171b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_iob2_file(ME_test, path=\"../data/me_data/middle_eastern_test.iob2\", gold=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
