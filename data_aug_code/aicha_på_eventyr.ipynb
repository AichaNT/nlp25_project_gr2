{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from ME_BPER import ME_BPER\n",
    "from ME_IPER import extract_last_names\n",
    "from ME_LOC import add_location\n",
    "from ME_ORG import add_organisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading label data from a given column\n",
    "# this is the readNlu function from the provided span_f1 file\n",
    "# minor modifications were made to make it usable with our data. \n",
    "def readNlu(path, target_column = 1): # default to index 1 (thats where DaN+ labels are)\n",
    "    '''\n",
    "    This function reads labeled annotations from a CoNLL-like file.\n",
    "\n",
    "    It parses a file where each line typically represents a single token and its annotations,\n",
    "    separated by tabs. Empty lines denote sentence boundaries. It extracts labels from a specified column\n",
    "    (by default, column index 1), collecting them as a list of label sequences, one per sentence.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the input file.\n",
    "        target_column (int, optional): Index of the column to extract labels from. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: A list where each element is a list of labels (strings) corresponding\n",
    "                         to tokens in a sentence.\n",
    "    '''\n",
    "\n",
    "    annotations = []    # list for storing all the label sequences (one per sentence)\n",
    "    cur_annotation = [] # temp list for labels of the current sentence\n",
    "\n",
    "    # reading through the file line by line\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()                     # remove leading/trailing whitespaces\n",
    "\n",
    "        # empty lines denotes end of sentence\n",
    "        if line == '':\n",
    "            annotations.append(cur_annotation)  # add current annotations to annotations list\n",
    "            cur_annotation = []                 # reset for the next sentence\n",
    "        \n",
    "        # skipping comments (start with \"#\" and no tokens columns)\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            # extract the label from the specified column and add to current sentence\n",
    "            cur_annotation.append(line.split('\\t')[target_column])\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# mapping funciton \n",
    "def mapping(path):\n",
    "    '''\n",
    "    This function generates mappings between labels and their corresponding integer IDs from a labeled dataset.\n",
    "\n",
    "    It reads annotations from a CoNLL-like file using the `readNlu` function,\n",
    "    filters out labels containing substrings like \"part\" or \"deriv\" (case-insensitive),\n",
    "    and creates a bidirectional mapping between the remaining unique labels and integer IDs.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the labeled data file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], Dict[int, str]]:\n",
    "            - label2id: A dictionary mapping each label to a unique integer ID.\n",
    "            - id2label: A reverse dictionary mapping each integer ID back to its label.\n",
    "    '''\n",
    "\n",
    "    # get the data labels\n",
    "    data_labels = readNlu(path) \n",
    "\n",
    "    # create empty set to store unique labels\n",
    "    label_set = set()\n",
    "\n",
    "    for labels in data_labels:\n",
    "        #  filter out any labels that contain 'part' or 'deriv' (case-insensitive)\n",
    "        filtered = [label for label in labels if 'part' not in label.lower() and 'deriv' not in label.lower()]\n",
    "        label_set.update(filtered)\n",
    "\n",
    "    # count of unique filtered labels\n",
    "    num_labels = len(label_set)\n",
    "\n",
    "    # create a dictionary mapping each label to a unique integer ID\n",
    "    label2id = {label: id for id, label in enumerate(label_set)}\n",
    "\n",
    "    # create a dictionary mapping each unique integer ID to a label\n",
    "    id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "# load data function\n",
    "# heavily inspired by the solution from assignment 5\n",
    "def read_tsv_file(path, label2id):\n",
    "    '''\n",
    "    This function reads a TSV file containing tokens and NER labels and converts it into structured data.\n",
    "    It collects the tokens, their original labels, and their corresponding integer IDs (based on the provided `label2id` mapping) for each sentence.\n",
    "    Sentences are separated by empty lines. \n",
    "\n",
    "    Each non-empty line in the file is expected to have at least two tab-separated columns:\n",
    "    - The first column is the token.\n",
    "    - The second column is the corresponding NER label.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the TSV file to read.\n",
    "        label2id (dict): A dictionary mapping NER label strings to their corresponding integer IDs.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries, one per sentence, with keys:\n",
    "            - 'tokens': list of tokens.\n",
    "            - 'ner_tags': list of original NER label strings.\n",
    "            - 'tag_ids': list of integer tag IDs corresponding to the NER labels.\n",
    "    '''\n",
    "\n",
    "    data = []               # final list to hold all sentences as dictionaries\n",
    "    current_words = []      # tokens for the current sentence\n",
    "    current_tags = []       # NER tags for the current sentence\n",
    "    current_tag_ids = []    # corresponding tag IDs for the current sentence\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip() # removes any leading and trailing whitespaces from the line\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#': \n",
    "                continue # skip comments\n",
    "\n",
    "            # splitting at 'tab', as the data is tab separated \n",
    "            tok = line.split('\\t')\n",
    "            \n",
    "            # extract the token (first column)\n",
    "            token = tok[0]\n",
    "\n",
    "            # check if the label is in the provided label2id dictionary\n",
    "            # if it's not, replace the label with 'O'\n",
    "            label = tok[1] if tok[1] in label2id else 'O'\n",
    "\n",
    "            current_words.append(token)\n",
    "            current_tags.append(label)\n",
    "            current_tag_ids.append(label2id[label])\n",
    "        \n",
    "        else: # skip empty lines\n",
    "            if current_words: # if current_words is not empty\n",
    "\n",
    "                # add entry to dict where tokens and ner_tags are keys and the values are lists\n",
    "                data.append({\"tokens\": current_words, \"ner_tags\": current_tags, \"tag_ids\": current_tag_ids})\n",
    "\n",
    "            # start over  \n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "            current_tag_ids = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append({\"tokens\": current_words, \"ner_tags\": current_tags, \"tag_ids\": current_tag_ids})\n",
    "  \n",
    "    return data\n",
    "\n",
    "# extracting tokens to check for overlap in train, dev and test sets\n",
    "def extract_labeled_tokens(dataset, exclude_label = \"O\", include_label_pair=False):\n",
    "    '''\n",
    "    This function extracts tokens from a dataset that have a string label different from `exclude_label`.\n",
    "    Optionally, it can return the (token, label) pairs instead of just tokens.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (List[dict]): The token-tagged dataset.\n",
    "        exclude_label (str): The label to ignore (default is 'O').\n",
    "        include_label_pair (bool): Whether to include the (token, label) pairs in the result (default is False).\n",
    "        \n",
    "    Returns:\n",
    "         Set[str] or Set[Tuple[str, str]]: \n",
    "            - A set of tokens with meaningful (non-O) labels if `include_label_pair` is False.\n",
    "            - A set of (token, label) pairs if `include_label_pair` is True.\n",
    "    '''\n",
    "\n",
    "    # create empty set to store the unique tokens\n",
    "    labeled_tokens = set()\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        # iterate over each token and its corresponding tag ID\n",
    "        for token, label in zip(sentence[\"tokens\"], sentence[\"ner_tags\"]):\n",
    "            if label != exclude_label:                      # check if the tag is not the excluded one\n",
    "                if include_label_pair:\n",
    "                    labeled_tokens.add((token, label))      # add (token, label) pair if the flag is True\n",
    "                else:\n",
    "                    labeled_tokens.add(token)               # add just the token if the flag is False\n",
    "    \n",
    "    return labeled_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_aug_sources/the-middle-east-cities.csv\", sep = \";\", skiprows = 1)\n",
    "unique_city_da = df[\"city_da\"].drop_duplicates()\n",
    "ME_LOC = [add_location(loc) for loc in unique_city_da]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_aug_sources/middle_eastern_organisations.csv\", sep = \";\", skiprows = 1)\n",
    "unique_orgs = df[\"org\"].drop_duplicates()\n",
    "ME_ORG = [add_organisation(org) for org in unique_orgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_entity_strings(data, target_label_prefix):\n",
    "    \"\"\"\n",
    "    Collects labeled spans as strings from the dataset. Multi-token spans are joined with spaces.\n",
    "    \n",
    "    Args:\n",
    "        data (List[Dict]): Dataset containing 'tokens' and 'tags' for each sentence.\n",
    "        target_label_prefix (str): Label prefix to filter for (e.g., 'B-LOC', 'B-ORG').\n",
    "        \n",
    "    Returns:\n",
    "        Set[str]: A set of labeled token strings (e.g., {'Beirut', 'Al Mawsil al Jadidah'})\n",
    "    \"\"\"\n",
    "    grouped_strings = set()\n",
    "\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        tags = item['ner_tags']\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tag = tags[i]\n",
    "\n",
    "            if tag.startswith(target_label_prefix):\n",
    "                span_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                while i < len(tokens) and tags[i].startswith('I'):\n",
    "                    span_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "                # Join tokens into a single string and add to the set\n",
    "                entity_string = ' '.join(span_tokens)\n",
    "                grouped_strings.add(entity_string)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return grouped_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ner_tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# overlap between train, dev, test and MENAPT NEs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ME_LOC_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_entity_strings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mME_LOC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_label_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB-LOC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ME_ORG_tokens \u001b[38;5;241m=\u001b[39m collect_entity_strings(ME_ORG, target_label_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-ORG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(ME_LOC_tokens)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mcollect_entity_strings\u001b[0;34m(data, target_label_prefix)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     15\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mner_tags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ner_tags'"
     ]
    }
   ],
   "source": [
    "# overlap between train, dev, test and MENAPT NEs\n",
    "ME_LOC_tokens = collect_entity_strings(ME_LOC, target_label_prefix = \"B-LOC\")\n",
    "\n",
    "ME_ORG_tokens = collect_entity_strings(ME_ORG, target_label_prefix = \"B-ORG\")\n",
    "\n",
    "print(ME_LOC_tokens)\n",
    "print(len(ME_LOC_tokens))\n",
    "print(len(ME_LOC))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(ME_ORG_tokens)\n",
    "print(len(ME_ORG_tokens))\n",
    "print(len(ME_ORG))\n",
    "\n",
    "ME_BPER_tokens = set(ME_BPER)\n",
    "#ME_IPER_tokens = set(ME_IPER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_train = \"../data/da_news_train.tsv\"\n",
    "path_dev = \"../data/da_news_dev.tsv\"\n",
    "path_test = \"../data/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_entity_strings(data, exclude_label=\"O\"):\n",
    "    \"\"\"\n",
    "    Collects all labeled (non-\"O\") entity spans as strings from the dataset.\n",
    "    Multi-token spans are joined with spaces.\n",
    "\n",
    "    Args:\n",
    "        data (List[Dict]): Dataset with 'tokens' and 'tags' per sentence.\n",
    "        exclude_label (str): Label to ignore (default is 'O').\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: Set of labeled entity strings (e.g., {'Beirut', 'Al Mawsil al Jadidah'})\n",
    "    \"\"\"\n",
    "    grouped_strings = set()\n",
    "\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        tags = item['ner_tags']\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tag = tags[i]\n",
    "\n",
    "            if tag != exclude_label and tag.startswith('B-'):\n",
    "                span_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                # Collect I-XXX continuation tags\n",
    "                while i < len(tokens) and tags[i].startswith('I-'):\n",
    "                    span_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "                entity_string = ' '.join(span_tokens)\n",
    "                grouped_strings.add(entity_string)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return grouped_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = collect_all_entity_strings(train_data)\n",
    "dev_tokens = collect_all_entity_strings(dev_data)\n",
    "test_tokens = collect_all_entity_strings(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap loc train:  {'Bush', 'Irak', 'Ankara', 'Abu Dhabi', 'Syrien', 'Luxor', 'Erzincan', 'Kuwait'}\n",
      "overlap loc dev:  {'Bahrain', 'Oman'}\n",
      "overlap loc test:  {'Irak', 'Bagdad'}\n",
      "overlap org train:  {'CBC'}\n",
      "overlap org dev:  {'CBC'}\n",
      "overlap org test:  set()\n",
      "overlap BPER train:  {'S', 'Bassam', 'Elias', 'K'}\n",
      "overlap BPER dev:  set()\n",
      "overlap BPER test:  {'Z', 'K'}\n"
     ]
    }
   ],
   "source": [
    "print(\"overlap loc train: \", train_tokens & ME_LOC_tokens)\n",
    "print(\"overlap loc dev: \", dev_tokens & ME_LOC_tokens)\n",
    "print(\"overlap loc test: \", test_tokens & ME_LOC_tokens)\n",
    "\n",
    "print(\"overlap org train: \", train_tokens & ME_ORG_tokens)\n",
    "print(\"overlap org dev: \", dev_tokens & ME_ORG_tokens)\n",
    "print(\"overlap org test: \", test_tokens & ME_ORG_tokens)\n",
    "\n",
    "print(\"overlap BPER train: \", train_tokens & ME_BPER_tokens)\n",
    "print(\"overlap BPER dev: \", dev_tokens & ME_BPER_tokens)\n",
    "print(\"overlap BPER test: \", test_tokens & ME_BPER_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entity_span_overlap(data, overlap_entities, is_list_of_dicts=True):\n",
    "    \"\"\"\n",
    "    Removes overlapping entities from either:\n",
    "    - A list of dicts with 'tokens' and 'tags' (for NER-style data).\n",
    "    - A simple list of entity strings (for name lists like ME_BPER or ME_IPER).\n",
    "\n",
    "    Args:\n",
    "        data (List[Dict] or List[str]): The dataset to filter.\n",
    "        overlap_entities (Set[str]): Set of full entity spans to exclude.\n",
    "        is_list_of_dicts (bool): Set True if data is NER-style with tokens/tags, False if it's a simple list of strings.\n",
    "\n",
    "    Returns:\n",
    "        List: Filtered dataset with overlaps removed.\n",
    "    \"\"\"\n",
    "\n",
    "    if not is_list_of_dicts:\n",
    "        # Data is a list of entity strings (e.g., ME_BPER)\n",
    "        return [entity for entity in data if entity not in overlap_entities]\n",
    "\n",
    "    # Helper: extract entity spans from token/tag lists\n",
    "    def extract_entity_spans(tokens, tags):\n",
    "        spans = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if tags[i].startswith(\"B-\"):\n",
    "                span_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                while i < len(tokens) and tags[i].startswith(\"I-\"):\n",
    "                    span_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "                spans.append(\" \".join(span_tokens))\n",
    "            else:\n",
    "                i += 1\n",
    "        return set(spans)\n",
    "\n",
    "    # NER-style case: remove full examples that contain overlapping spans\n",
    "    filtered_data = []\n",
    "    for item in data:\n",
    "        spans = extract_entity_spans(item[\"tokens\"], item[\"ner_tags\"])\n",
    "        if not spans & overlap_entities:\n",
    "            filtered_data.append(item)\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m overlap_loc_spans \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mcollect_entity_strings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB-LOC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m      3\u001b[0m     collect_entity_strings(dev_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-LOC\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m      4\u001b[0m     collect_entity_strings(test_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-LOC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m overlap_org_spans \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      8\u001b[0m     collect_entity_strings(train_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-ORG\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m      9\u001b[0m     collect_entity_strings(dev_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-ORG\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m     10\u001b[0m     collect_entity_strings(test_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-ORG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m overlap_BPER_spans \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     train_tokens \u001b[38;5;241m&\u001b[39m ME_BPER_tokens \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m     15\u001b[0m     dev_tokens \u001b[38;5;241m&\u001b[39m ME_BPER_tokens \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m     16\u001b[0m     test_tokens \u001b[38;5;241m&\u001b[39m ME_BPER_tokens\n\u001b[1;32m     17\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mcollect_entity_strings\u001b[0;34m(data, target_label_prefix)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     15\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tags'"
     ]
    }
   ],
   "source": [
    "overlap_loc_spans = (\n",
    "    collect_entity_strings(train_data, \"B-LOC\") |\n",
    "    collect_entity_strings(dev_data, \"B-LOC\") |\n",
    "    collect_entity_strings(test_data, \"B-LOC\")\n",
    ")\n",
    "\n",
    "overlap_org_spans = (\n",
    "    collect_entity_strings(train_data, \"B-ORG\") |\n",
    "    collect_entity_strings(dev_data, \"B-ORG\") |\n",
    "    collect_entity_strings(test_data, \"B-ORG\")\n",
    ")\n",
    "\n",
    "overlap_BPER_spans = (\n",
    "    train_tokens & ME_BPER_tokens |\n",
    "    dev_tokens & ME_BPER_tokens |\n",
    "    test_tokens & ME_BPER_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For list of dicts (e.g., ME_LOC or ME_ORG)\n",
    "updated_ME_LOC = remove_entity_span_overlap(ME_LOC, overlap_loc_spans, is_list_of_dicts=True)\n",
    "print(\"Before ME_LOC:\", len(ME_LOC))\n",
    "print(\"After ME_LOC:\", len(updated_ME_LOC))\n",
    "\n",
    "# For list of dicts (e.g., ME_LOC or ME_ORG)\n",
    "updated_ME_ORG = remove_entity_span_overlap(ME_ORG, overlap_org_spans, is_list_of_dicts=True)\n",
    "print(\"Before ME_ORG:\", len(ME_ORG))\n",
    "print(\"After ME_ORG:\", len(updated_ME_ORG))\n",
    "\n",
    "# For simple list (e.g., ME_BPER)\n",
    "updated_ME_BPER = remove_entity_span_overlap(ME_BPER, overlap_bper_spans, is_list_of_dicts=False)\n",
    "print(\"Before ME_BPER:\", len(ME_BPER))\n",
    "print(\"After ME_BPER:\", len(updated_ME_BPER))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap loc train:  {'Abu Dhabi'}\n",
      "overlap loc dev:  set()\n",
      "overlap loc test:  set()\n",
      "overlap org train:  set()\n",
      "overlap org dev:  set()\n",
      "overlap org test:  set()\n",
      "overlap BPER train:  set()\n",
      "overlap BPER dev:  set()\n",
      "overlap BPER test:  set()\n"
     ]
    }
   ],
   "source": [
    "print(\"overlap loc train: \", train_tokens & ME_LOC_tokens)\n",
    "print(\"overlap loc dev: \", dev_tokens & ME_LOC_tokens)\n",
    "print(\"overlap loc test: \", test_tokens & ME_LOC_tokens)\n",
    "\n",
    "print(\"overlap org train: \", train_tokens & ME_ORG_tokens)\n",
    "print(\"overlap org dev: \", dev_tokens & ME_ORG_tokens)\n",
    "print(\"overlap org test: \", test_tokens & ME_ORG_tokens)\n",
    "\n",
    "print(\"overlap BPER train: \", train_tokens & set(updated_ME_BPER_tokens))\n",
    "print(\"overlap BPER dev: \", dev_tokens & set(updated_ME_BPER_tokens))\n",
    "print(\"overlap BPER test: \", test_tokens & set(updated_ME_BPER_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount):\n",
    "    # First, filter sentences that have at least one non-\"O\" tag\n",
    "    eligible_sentences = [sent for sent in dataset if any(tag != \"O\" for tag in sent[\"ner_tags\"])]\n",
    "\n",
    "    # Select up to sentence_amount randomly from the eligible ones\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "\n",
    "\n",
    "    for sent in selected_sentences:\n",
    "        i = 0\n",
    "\n",
    "        while i<len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                replace = random.choice(ME_BPER)\n",
    "                sent[\"ner_tags\"][i] = replace\n",
    "                ME_BPER.remove(replace)\n",
    "                i+=1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                replace = random.choice(ME_IPER)\n",
    "                sent[\"ner_tags\"][i] = replace\n",
    "                ME_IPER.remove(replace)\n",
    "                i+=1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                same_length_LOC = []\n",
    "                for loc in ME_LOC:\n",
    "                    if len(loc) == span_len:\n",
    "                        same_length_LOC.append(loc)\n",
    "                if same_length_LOC:\n",
    "                    replace = random.choice(same_length_LOC)\n",
    "                    sent[\"tokens\"][span_start:span_start+span_len] = replace\n",
    "                    ME_LOC.remove(replace)\n",
    "            \n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                same_length_ORG = []\n",
    "                for org in ME_ORG:\n",
    "                    if len(org) == span_len:\n",
    "                        same_length_ORG.append(org)\n",
    "                if same_length_ORG:\n",
    "                    replace = random.choice(same_length_ORG)\n",
    "                    sent[\"tokens\"][span_start:span_start+span_len] = replace\n",
    "                    ME_ORG.remove(replace)\n",
    "\n",
    "            else:\n",
    "                i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2143288949.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dev_data =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dev_data = \n",
    "what = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
