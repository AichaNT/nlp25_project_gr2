{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from ME_BPER import ME_BPER\n",
    "from ME_IPER import extract_last_names\n",
    "from ME_LOC import add_location\n",
    "from ME_ORG import add_organisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading label data from a given column\n",
    "# this is the readNlu function from the provided span_f1 file\n",
    "# minor modifications were made to make it usable with our data. \n",
    "def readNlu(path, target_column = 1): # default to index 1 (thats where DaN+ labels are)\n",
    "    '''\n",
    "    This function reads labeled annotations from a CoNLL-like file.\n",
    "\n",
    "    It parses a file where each line typically represents a single token and its annotations,\n",
    "    separated by tabs. Empty lines denote sentence boundaries. It extracts labels from a specified column\n",
    "    (by default, column index 1), collecting them as a list of label sequences, one per sentence.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the input file.\n",
    "        target_column (int, optional): Index of the column to extract labels from. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: A list where each element is a list of labels (strings) corresponding\n",
    "                         to tokens in a sentence.\n",
    "    '''\n",
    "\n",
    "    annotations = []    # list for storing all the label sequences (one per sentence)\n",
    "    cur_annotation = [] # temp list for labels of the current sentence\n",
    "\n",
    "    # reading through the file line by line\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()                     # remove leading/trailing whitespaces\n",
    "\n",
    "        # empty lines denotes end of sentence\n",
    "        if line == '':\n",
    "            annotations.append(cur_annotation)  # add current annotations to annotations list\n",
    "            cur_annotation = []                 # reset for the next sentence\n",
    "        \n",
    "        # skipping comments (start with \"#\" and no tokens columns)\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            # extract the label from the specified column and add to current sentence\n",
    "            cur_annotation.append(line.split('\\t')[target_column])\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# mapping funciton \n",
    "def mapping(path):\n",
    "    '''\n",
    "    This function generates mappings between labels and their corresponding integer IDs from a labeled dataset.\n",
    "\n",
    "    It reads annotations from a CoNLL-like file using the `readNlu` function,\n",
    "    filters out labels containing substrings like \"part\" or \"deriv\" (case-insensitive),\n",
    "    and creates a bidirectional mapping between the remaining unique labels and integer IDs.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the labeled data file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], Dict[int, str]]:\n",
    "            - label2id: A dictionary mapping each label to a unique integer ID.\n",
    "            - id2label: A reverse dictionary mapping each integer ID back to its label.\n",
    "    '''\n",
    "\n",
    "    # get the data labels\n",
    "    data_labels = readNlu(path) \n",
    "\n",
    "    # create empty set to store unique labels\n",
    "    label_set = set()\n",
    "\n",
    "    for labels in data_labels:\n",
    "        #  filter out any labels that contain 'part' or 'deriv' (case-insensitive)\n",
    "        filtered = [label for label in labels if 'part' not in label.lower() and 'deriv' not in label.lower()]\n",
    "        label_set.update(filtered)\n",
    "\n",
    "    # count of unique filtered labels\n",
    "    num_labels = len(label_set)\n",
    "\n",
    "    # create a dictionary mapping each label to a unique integer ID\n",
    "    label2id = {label: id for id, label in enumerate(label_set)}\n",
    "\n",
    "    # create a dictionary mapping each unique integer ID to a label\n",
    "    id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "# load data function\n",
    "# heavily inspired by the solution from assignment 5\n",
    "def read_tsv_file(path, label2id):\n",
    "    '''\n",
    "    This function reads a TSV file containing tokens and NER labels and converts it into structured data.\n",
    "    It collects the tokens, their original labels, and their corresponding integer IDs (based on the provided `label2id` mapping) for each sentence.\n",
    "    Sentences are separated by empty lines. \n",
    "\n",
    "    Each non-empty line in the file is expected to have at least two tab-separated columns:\n",
    "    - The first column is the token.\n",
    "    - The second column is the corresponding NER label.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the TSV file to read.\n",
    "        label2id (dict): A dictionary mapping NER label strings to their corresponding integer IDs.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries, one per sentence, with keys:\n",
    "            - 'tokens': list of tokens.\n",
    "            - 'ner_tags': list of original NER label strings.\n",
    "            - 'tag_ids': list of integer tag IDs corresponding to the NER labels.\n",
    "    '''\n",
    "\n",
    "    data = []               # final list to hold all sentences as dictionaries\n",
    "    current_words = []      # tokens for the current sentence\n",
    "    current_tags = []       # NER tags for the current sentence\n",
    "    current_tag_ids = []    # corresponding tag IDs for the current sentence\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip() # removes any leading and trailing whitespaces from the line\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#': \n",
    "                continue # skip comments\n",
    "\n",
    "            # splitting at 'tab', as the data is tab separated \n",
    "            tok = line.split('\\t')\n",
    "            \n",
    "            # extract the token (first column)\n",
    "            token = tok[0]\n",
    "\n",
    "            # check if the label is in the provided label2id dictionary\n",
    "            # if it's not, replace the label with 'O'\n",
    "            label = tok[1] if tok[1] in label2id else 'O'\n",
    "\n",
    "            current_words.append(token)\n",
    "            current_tags.append(label)\n",
    "            current_tag_ids.append(label2id[label])\n",
    "        \n",
    "        else: # skip empty lines\n",
    "            if current_words: # if current_words is not empty\n",
    "\n",
    "                # add entry to dict where tokens and ner_tags are keys and the values are lists\n",
    "                data.append({\"tokens\": current_words, \"ner_tags\": current_tags, \"tag_ids\": current_tag_ids})\n",
    "\n",
    "            # start over  \n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "            current_tag_ids = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append({\"tokens\": current_words, \"ner_tags\": current_tags, \"tag_ids\": current_tag_ids})\n",
    "  \n",
    "    return data\n",
    "\n",
    "# extracting tokens to check for overlap in train, dev and test sets\n",
    "def extract_labeled_tokens(dataset, exclude_label = \"O\", include_label_pair=False):\n",
    "    '''\n",
    "    This function extracts tokens from a dataset that have a string label different from `exclude_label`.\n",
    "    Optionally, it can return the (token, label) pairs instead of just tokens.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (List[dict]): The token-tagged dataset.\n",
    "        exclude_label (str): The label to ignore (default is 'O').\n",
    "        include_label_pair (bool): Whether to include the (token, label) pairs in the result (default is False).\n",
    "        \n",
    "    Returns:\n",
    "         Set[str] or Set[Tuple[str, str]]: \n",
    "            - A set of tokens with meaningful (non-O) labels if `include_label_pair` is False.\n",
    "            - A set of (token, label) pairs if `include_label_pair` is True.\n",
    "    '''\n",
    "\n",
    "    # create empty set to store the unique tokens\n",
    "    labeled_tokens = set()\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        # iterate over each token and its corresponding tag ID\n",
    "        for token, label in zip(sentence[\"tokens\"], sentence[\"ner_tags\"]):\n",
    "            if label != exclude_label:                      # check if the tag is not the excluded one\n",
    "                if include_label_pair:\n",
    "                    labeled_tokens.add((token, label))      # add (token, label) pair if the flag is True\n",
    "                else:\n",
    "                    labeled_tokens.add(token)               # add just the token if the flag is False\n",
    "    \n",
    "    return labeled_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_aug_sources/the-middle-east-cities.csv\", sep = \";\", skiprows = 1)\n",
    "unique_city_da = df[\"city_da\"].drop_duplicates()\n",
    "ME_LOC = [add_location(loc) for loc in unique_city_da]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_aug_sources/middle_eastern_organisations.csv\", sep = \";\", skiprows = 1)\n",
    "unique_orgs = df[\"org\"].drop_duplicates()\n",
    "ME_ORG = [add_organisation(org) for org in unique_orgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_entity_strings(data, target_label_prefix):\n",
    "    \"\"\"\n",
    "    Collects labeled spans as strings from the dataset. Multi-token spans are joined with spaces.\n",
    "    \n",
    "    Args:\n",
    "        data (List[Dict]): Dataset containing 'tokens' and 'tags' for each sentence.\n",
    "        target_label_prefix (str): Label prefix to filter for (e.g., 'B-LOC', 'B-ORG').\n",
    "        \n",
    "    Returns:\n",
    "        Set[str]: A set of labeled token strings (e.g., {'Beirut', 'Al Mawsil al Jadidah'})\n",
    "    \"\"\"\n",
    "    grouped_strings = set()\n",
    "\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        tags = item['ner_tags']\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tag = tags[i]\n",
    "\n",
    "            if tag.startswith(target_label_prefix):\n",
    "                span_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                while i < len(tokens) and tags[i].startswith('I'):\n",
    "                    span_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "                # Join tokens into a single string and add to the set\n",
    "                entity_string = ' '.join(span_tokens)\n",
    "                grouped_strings.add(entity_string)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return grouped_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'New Cairo', 'Bukan', 'Yazd', 'Malatya', 'Najafabad', 'Bam', 'Hamadan', 'Erzurum', 'Ajman', 'Latakia', 'Dammam', 'Muratpasa', 'Bingöl', 'Ibri', 'Ar Ramtha', 'Qom', 'Çanakkale', 'Umm Qasr', 'Ibb', 'Rize', 'Ajlun', 'Khorramabad', 'Belek', 'Kirkuk', 'Zahle', 'Adana', 'Al Mansurah', 'Bandar-e Anzali', 'Üsküdarr', 'Midyat', 'Jabalya', 'Eskisehir', 'Bonab', 'Baqubah', 'Chalus', 'Karabük', 'Damanhur', 'Kayseri', 'Antalya', 'Iğdır', 'Soma', 'Shushtar', 'Bishah', 'Ras Beirut', 'Qina', 'Al Muharraq', 'Edfu', 'Bolu', 'Kuwait', 'Al Matariyah', 'Ceyhan', 'Iskenderun', 'Fasa', 'Mosul', 'Elbistan', 'Aqaba', 'Damaskus', 'Trabzon', 'Kerman', 'Zeytinburnu', 'Abu Dhabi', 'Sabzevar', 'Habbouch', 'Mashhad', 'Silifke', 'Sohar', 'Basra', 'Al Fallujah', 'Manfalut', 'Muscat', 'Barka', 'Şanlıurfa', 'Borazjan', \"Ta'if\", 'Shahrud', 'Tabuk', 'Ilam', 'As Salamiyah', 'Doha', 'Zabol', 'Diyarbakir', 'Ismailia', 'Şişli', 'Borujerd', 'Balikesir', 'Dhahran', 'Al Mubarraz', 'Gemlik', 'Sirjan', 'Gonbad-e Kavus', 'Ordu', 'Afyonkarahisar', 'Aligudarz', 'Ar Raqqah', 'Deir ez-Zor', 'Kashmar', 'Isfahan', 'Birjand', 'Batman', 'Akbarabad', 'Shibin al Kawm', 'Rafsanjan', 'Khash', \"A'zaz\", 'Kars', 'Mustafakemalpaşa', 'Adiyaman', 'Konya', 'Turgutlu', 'Amol', 'Tatvan', 'Mersa Matruh', 'Kufa', 'Turhal', 'Qorveh', 'Kuhdasht', 'Tarsus', 'Hamah', 'Uşak', 'Daraa', 'Nusaybin', 'Corlu', 'Adapazari', 'Ümraniye', 'Awsim', 'Atasehir', 'Sabah as Salim', 'Ahvaz', 'Ar Rayyan', 'Al Fashn', 'Giza', 'Elazig', 'Ad Diwaniyah', 'Edirne', 'Firuzabad', 'Marivan', 'Homs', 'Azadshahr', 'Erciş', 'Sulaymaniyah', 'Söke', 'As Salimiyah', 'Al Basrah al Qadimah', 'Tanda', 'Gorgan', 'Anzal-e Jonubi', 'Zagazig', 'Fatsa', 'Polatlı', 'Saqqez', 'Çerkezköy', 'Orumiyeh', 'Büyükçekmece', 'Varamin', 'Al Minya', 'Arak', 'Al Ain', 'Bismil', 'Giresun', 'Siirt', 'Sultanbeyli', 'Hawalli', 'Merkezefendi', 'Manbij', 'Kozan', 'Al Mahallah al Kubra', 'Antakya', 'Kahramanmaraş', 'Babol', 'Shahre Jadide Andisheh', 'Bawshar', 'Alvand', 'Bağcılar', 'Tokat', 'Isparta', 'Esenyurt', 'Rafah', 'Ar Rass', 'Az Zubayr', 'Samalut', 'Isna', 'Yalova', 'Halwan', 'Van', 'Zefta', 'Ahlat', 'Gaziantep', 'Jizan', 'Doğubayazıt', 'Al-Hasakah', 'Yüksekova', 'Luxor', 'Bafra', 'Salalah', 'Batikent', 'Aydin', 'Bilbeis', 'Arar', 'Iran', 'Qalyub', 'Ardabil', 'Sohag', 'Bandırma', 'Ar Rifa', 'Masjed Soleyman', 'Al Kharj', 'Nabatiye et Tahta', \"Al 'Amarah\", 'Kairo', \"Ha'il\", 'Silvan', 'Abu Ghurayb', 'Toukh', 'Russeifa', 'Khobar', 'Khamis Mushait', 'Dahuk', 'Ras al-Khaimah', 'Talkha', 'İnegöl', 'Herzliya', 'Başakşehir', 'Kutahya', 'Amman', 'Jamjamal', 'Al Mawsil al Jadidah', 'Niğde', 'Sorgun', 'Bojnurd', 'Wadi as Sir', 'Ash Shafa', 'Forenede Arabiske Emirater', 'Minab', 'Akhmim', 'Bani Suwayf', 'Yozgat', 'Ünye', 'Zanjan', 'Maltepe', 'Kahriz', 'Al-Arish', 'Kilis', 'Khanjarah', 'Faqus', 'Darayya', 'Housh Eissa', 'Tripoli', 'Buraydah', 'Sinah', 'Suez', 'Khomeyni Shahr', 'Zaxo', 'Ereğli', 'Jidda', 'Al Fujayrah', 'Manama', 'Jounieh', 'Sanaá', 'Al Kharijah', 'Al Hayy', 'Beylikduezue', 'Abha', 'Munuf', 'Sanandaj', 'Bahçelievler', 'Izmit', 'Bilqas', 'Riyadh', 'Al Manzalah', 'Dubai', 'Al Harithah', 'Egypten', 'Sultangazi', 'Zarqa', 'Khomeyn', 'Sultanah', 'Khalis', 'Karabağlar', 'As Suwayq', 'Kırşehir', 'Ardeşen', 'Torbat-e Heydariyeh', 'Salihli', 'Ayvalık', 'Dar Kulayb', 'Mallawi', 'Tartus', 'Bushehr', 'Nahavand', 'Behshahr', 'Dayrut', 'Disuq', 'Syrien', 'Seeb', 'Hebron', 'Iranshahr', 'Abu Tij', 'Nurabad', 'Kastamonu', 'Erzincan', 'Behbahan', 'Yasuj', 'Baneh', 'Manisa', 'Denizli', 'Istanbul', 'Aden', 'Esenler', 'Kazerun', 'Fuwwah', 'Al Hawamidiyah', 'Piranshahr', 'Ramadi', 'Al Kut', 'Idlib', 'Marand', 'Robat Karim', 'Çorum', 'Irak', 'Khoy', 'Sancaktepe', 'Shirvan', 'Al Buraymi', 'Darab', 'Medina', 'Sari', 'Beirut', 'Rosetta', 'Douma', 'Port Said', 'Al-Hudaydah', 'Viranşehir', 'Saudi Arabien', 'Erbil', 'Tekirdağ', 'Khan Yunis', 'As-Salt', 'Qazvin', 'Aleppo', 'Hafar Al-Batin', 'Ath Thawrah', 'Burdur', 'Cizre', 'Taiz', 'Akhisar', 'Al Hillah', 'Jirja', 'Zahedan', 'Semnan', 'Dehdasht', 'Abnub', 'Sayhat', 'Ashmun', 'Shiraz', 'Ağrı', 'Kadirli', 'Teheran', 'Kuşadası', 'Karaj', 'Kafr az Zayyat', 'Saveh', 'Neyshabur', 'Samarra', 'Bagdad', 'Rustaq', 'Bandar Abbas', 'Østjerusalem', 'Najran', 'Sharjah', 'Damietta', 'Naqadeh', 'Mekka', 'Al Hufuf', 'Kafr ash Shaykh', 'Abadan', 'Gebze', 'Kirikkale', 'Khorramshahr', 'Patnos', 'Arnavutköy', 'Izmir', 'Al Faw', 'Hurghada', 'Körfez', 'Kermanshah', 'Idku', 'As Safirah', 'Gaza', 'Kafr ad Dawwar', 'Lüleburgaz', 'Muş', 'As Samawah', 'Silopi', 'Al Qusiyah', 'Rasht', 'Mukalla', 'Alanya', 'Shahr-e Kord', 'Sakakah', 'Banha', 'Al Ahmadi', 'Mardin', 'Mersin', 'Al Khankah', 'Oman', 'Nasiriyah', 'Damghan', 'Sayyan', 'Bahrain', 'Tahta', 'Al Jammaliyah', 'Yanbu', 'Saham', 'Bursa', 'Samsun', 'Langarud', 'Najaf', 'Sur', 'Nazarabad', 'Sidon', 'Aswan', 'Kızıltepe', 'Karbala', 'Salmas', 'Qurayyat', 'Ankara', 'Al Bahah', 'Çankaya', 'Rukban', 'Irbid', 'Dogonbadan', 'Osmaniye', 'Nizwa', 'Çubuk', 'Nablus', 'Kahta', 'Madaba', 'Ödemiş', 'Ahar', 'Alexandria', 'Takestan', 'Nevşehir', 'Al Qatif', 'Manavgat', 'Tabriz', 'Akşehir', 'Nazilli', 'Aksaray', 'Ash Shatrah', 'Hakkari', 'Parsabad', 'Sivas', 'Miandoab', 'Quchan', 'Siverek', 'Jordan', 'Malayer', 'Amran', 'Dikirnis', 'Al Bab', 'Bush', 'Asyut', 'Al Jubayl', 'Tyrkiet', 'Zonguldak', 'Karaman', 'Dhamar', 'Tarut', 'Palæstina', 'Nizip', 'Al Fayyum', 'Mahabad', 'Jablah', 'Tyre', 'Abhar', 'Libanon', 'Al Fahahil', 'Abu Kabir', 'Al Farwaniyah', 'Qarchak'}\n",
      "480\n",
      "481\n",
      "\n",
      "\n",
      "{'Al Shorouk', 'Deyaar', 'Gulf Bank', 'Spinneys', 'ONCF', 'AL24 News', 'Saudi Arabian Mining Company', 'al-Furat', 'Saudi Investment Bank', 'Virtuzone', 'Emirates Airline', 'Emsteel', 'Hawacom', 'Al Yah Satellite Communications', 'El Heddaf', 'Akhbar Nouakchott', 'Ma’aden', 'al-Balad', 'ONTV', 'Al Madina', 'Ad-Dustour', 'SABIC Agri-Nutrients Company', 'Sharjah Islamic Bank', 'Tunisavia', 'Al Ayam', 'Ooredoo', 'Al-Bilad', 'Emirates Islamic', 'Bank Of Africa', 'TAQA Group', 'Al Raya', 'Tamatem', 'Al-Mustaqbal', 'Emirates NBD', 'Al Kuwaitiya', 'Batelco', 'Koutoubia', 'Al-Nahar', 'Iqraa', 'Biougnach', 'Al Watan', 'al-Zahf Al-Akhdar', 'El Hiwar El Tounousi', 'Dukhan Bank', 'Arab Fund for Economic and Social Development', 'Sonasid', 'AD Ports Group', 'Doha Bank', 'Al-Anbaa', 'Kahramaa', 'Jawan', 'Altibbi', 'Al Jumhuriya', 'Al-Horria', 'Danube Group', 'al-Alam', 'Dubai Islamic Bank', 'Massaya', 'Aumet', 'Investcorp', 'Bayanat', 'Al-Intiqad', 'Roya TV', 'Al-Ahd Ul’Jadid', 'Al Jazeera', 'OSN', 'ZenHR Solutions', 'Qatar Fuel', 'ViaVii', 'Wasla', 'Alwasat', 'Sopriam', 'National Shipping Company of Saudi Arabia', 'Alinma Bank', 'Mashreq', 'NBB Group', 'Nareva', 'GEMS Education', 'Afriquia', 'Vermeg', 'Daily Sabah', 'Inagrab', 'The International Islamic Charitable Organization', 'Burgan Bank Group', 'Elm', 'Yallacompare', 'GIB Capital', 'QNB ALAHLI', 'Bank Albilad', 'Abu Dhabi Islamic Bank', 'ADES Holding', 'Al Manar', 'AKKASA', 'Tishreen', 'Mazazikh', 'Bahrain Islamic Bank', 'Derq', 'Adwya', 'Al Amal', 'NBN', 'Al-Ittihad', 'Al-Kalima', 'Marjane', 'Amal Glass', 'AvidBeam', 'Adaraweesh', 'Commercial International Bank', 'Kayhan Al Arabi', 'Al-Tijari', 'Saudi Aramco Base Oil Company', 'Al Nabooda Automobiles', 'BMCI', 'MarsaMaroc', 'BMMI', 'Cinescape', 'MBC', '360VUZ', 'Mo’men', 'International Holding Company', 'Asdaa Burson Marsteller', 'Banagas', 'The UAE’s Zayed Giving Initiative', 'Gulf Madhyamam', 'Alpha Dhabi Holding', 'Shihan', 'Jumeirah Group', 'Almarai', 'Wikaya', 'DEWA', 'Siger', 'Starworld', 'e&', 'Asiacell', 'National Bank of Kuwait', 'Al Rai', 'ANB', 'Bank ABC', 'Okaz', 'Dnata', 'Sahara International Petrochemical Company', 'BiscoMisr', 'Gulf News', 'El Mehwar', 'Al Rayyan', 'Etihad Etisalat Company', 'Al Mada', 'BulkWhiz', 'Laraki', 'SAMI', 'Popular Movement in Iraq', 'Ajeer', 'Marina Home', 'Cerebras', 'Nagham', 'Mazzika', 'Dussur', 'al-Watan', 'Athaqafia', 'Kuwait Oil Company', 'Jumeirah Hotels', 'Al-Waie', 'Electroplanet', 'Al Ahli Bank of Kuwait', 'Noon', 'Comarit', 'Somaca', 'Mathaqi', '2M', 'Mobily', 'Industries Qatar', 'ibTECHar Digital Solutions', 'Omantel', 'Cevital', 'NMDC Group', 'Sonatrach', 'Rezayat', 'Elves', 'Kezad', 'Ras Al Khaimah Economic Zone', 'Al-Kifah al-Arabi', 'Qatar Insurance Company', 'Saudi Awwal Bank', 'National Bank of Ras Al Khaimah', 'RasGas', 'Sonalgaz', 'Alrifai', 'ADCB Group', 'Arasco', 'Al Sharq', 'Arab Socialist Union Party of Syria', 'Mubadala', 'Muscat Stock Exchange', 'Dubai Electricity and Water Authority', 'Solfeh', 'Ashabiba', 'Istiqlal Party', 'al-Bayan', 'Kitea', 'al-Jamahiriyah', 'Marafiq', 'Télé Liban', 'Saudia', 'Argaam', 'Bimo', 'Borouge', 'Al-Mashriq', 'CBC', 'Jet4you', 'Nakilat', 'Kharafi Group', 'Fertiglobe', 'Al-Ahram', 'Qatar International Islamic Bank', 'National Covenant Party', 'Palestinian Liberation Front', 'Attijariwafa bank group', 'PureHealth Holding', 'The Noor Dubai Foundation', 'Siera', 'KIPCO', 'Oilibya', 'Abjjad', 'Khaleej Times', 'Al Aoula', 'Islamic Relief Worldwide', 'Managem', 'Qaym', 'Asmidal', 'Nehmeh', 'Mrsool', 'Ahlibank', 'Mawdoo3', 'Al Anbat', 'Careem', 'Khartoum', 'Arryadia', 'One', 'alrajhi bank', 'Assabeel', 'PSLab', 'Nabd', 'beIN SPORTS', 'The Syrian American Medical Society', 'Polisario Front', 'Al Massar', 'Akhbar Al Khaleej', 'QNB Group', 'Zain Group', 'Mauritel', 'Echorouk', 'al-Hayat al-Jadida', 'The Qatar Fund for Development', 'First Abu Dhabi Bank', 'Nouvelair', 'OTV', 'DenizBank', 'Maroc Telecom', 'QEWC', 'Tunisna', 'Egyptalum', 'JoSat', 'Bahri', 'Flynas', 'Du', 'Aramco', 'ACWA Power', 'Talabat', 'Wanasah', 'Emirates Integrated Telecommunications Company', 'Damac', 'Aldar Properties', 'Arab Bank', 'Kuwait Projects Company', 'Al Yaum', 'Amanat Holdings', 'Attajdid', 'Al-Mamlaka TV', 'Al-Ouruba', 'Kudu', 'Al-Muhaidib', 'Dr. Sulaiman Al Habib Medical Services Group', 'Aïcha', 'HalalaH', 'Ad Diyar', 'KTV2', 'Nadec', 'Al Tayer Group', 'al-Jarida al-Maghribia', 'Nasr', 'Zain KSA', 'ADNOC Gas', 'Saudi National Bank', 'Kuwait Finance House', 'Sarwa', 'Echorouk Group', 'Al-Massira', 'stc Group', 'Dubai Airports Company', 'Al Eqtisadiah', 'Al Anbaa', 'G42', 'Hawaï', 'Arab Democratic Nasserist Party', 'Inwi', 'Jamalon', 'al-Fajr al-Jadid', 'Thumbay Group', 'Al Rayaam', 'Aiguebelle', 'RASCO', 'Lamsa', 'Akdital', 'Air Arabia', '3ayez', 'Naseej', 'Bank AlJazira', 'LDC', 'Unifonic', 'Riyad Bank', 'Gulfsat', 'Daraty', 'GoEjaza', 'Qatar Charity', 'Qatar Islamic Bank', 'K24', 'Chefaa', 'Al Liwaa', 'Al Quds Association', 'Sela', 'Tarjama', 'Emaar Properties', 'The Khalifa Foundation', 'Baladna', 'The Palestinian Medical Relief Society', 'Al-Thawra', 'Swvl', 'al-Qabas', 'SOMED', 'The Company for Cooperative Insurance', 'Aster DM Healthcare', 'ADNOC', 'eSpace', 'Al-Wehda', 'Shamsina', 'Saudi Electricity Company', 'The Arab Medical Union', 'BCP Group', 'Aluminium Bahrain', 'Banque Saudi Fransi', 'Syriatel', 'Kingdom Holding Company', 'Colorado', 'The Jordan River Foundation', 'SNRT', 'Shawarmer', 'Filasteen al-Muslimah', 'Wallyscar', 'Bupa Arabia', 'Emirates Red Crescent', 'Bayane al-Yaoume', 'Bakdash', 'Sawani', 'LuLu Group', 'Assadissa', 'Djezzy', 'Etisalat', 'Ferrimaroc', 'Savola Group', 'ArabiaWeather', 'Epilert', 'Al-Arab Al-Yawm', 'Al Karmil', 'Libyana', 'Kharabeesh', 'Empower', 'Americana Restaurants', 'Ooredoo Group', 'Balad Party', 'Nilesat', 'EgyptAir', '218TV', 'Emaar Development', 'PubliTools', 'Arabot', 'SNVI', 'Dubai Investments', 'Alshaya', 'Eco-Médias', 'Mekameleen TV', 'The Ghassan Aboud Group', 'Evertek', 'Jordan Phosphate Mines Company', 'Al Jamahir', 'Boubyan Bank', 'Fajr Capital', 'Henkel GCC', 'Commercial Bank of Dubai', 'Agility', 'Saudi Aramco', 'Xenel', 'Patchi', 'Bank Muscat', 'Mondair', 'Torath', 'Souqalmal', 'Fetchr', 'SGTM', 'Saidal', 'DMC', 'Dabchy', 'Zaytouna TV', 'Shorooq Partners', 'Naftal', \"Arab Socialist Ba'ath Party\", 'Monoprix', 'arab national bank', 'Masraf Al Rayan', 'Cima', 'Commercial Bank', 'TGCC', 'Apparel Group', 'solutions by stc', 'Tabbah', 'National Bank of Fujairah', 'Lucidya', 'Omdurman', 'Malaeb'}\n",
      "427\n",
      "427\n"
     ]
    }
   ],
   "source": [
    "# overlap between train, dev, test and MENAPT NEs\n",
    "ME_LOC_tokens = collect_entity_strings(ME_LOC, target_label_prefix = \"B-LOC\")\n",
    "\n",
    "ME_ORG_tokens = collect_entity_strings(ME_ORG, target_label_prefix = \"B-ORG\")\n",
    "\n",
    "print(ME_LOC_tokens)\n",
    "print(len(ME_LOC_tokens))\n",
    "print(len(ME_LOC))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(ME_ORG_tokens)\n",
    "print(len(ME_ORG_tokens))\n",
    "print(len(ME_ORG))\n",
    "\n",
    "ME_BPER_tokens = set(ME_BPER)\n",
    "#ME_IPER_tokens = set(ME_IPER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_train = \"../data/da_news_train.tsv\"\n",
    "path_dev = \"../data/da_news_dev.tsv\"\n",
    "path_test = \"../data/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_entity_strings(data, exclude_label=\"O\"):\n",
    "    \"\"\"\n",
    "    Collects all labeled (non-\"O\") entity spans as strings from the dataset.\n",
    "    Multi-token spans are joined with spaces.\n",
    "\n",
    "    Args:\n",
    "        data (List[Dict]): Dataset with 'tokens' and 'tags' per sentence.\n",
    "        exclude_label (str): Label to ignore (default is 'O').\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: Set of labeled entity strings (e.g., {'Beirut', 'Al Mawsil al Jadidah'})\n",
    "    \"\"\"\n",
    "    grouped_strings = set()\n",
    "\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        tags = item['ner_tags']\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tag = tags[i]\n",
    "\n",
    "            if tag != exclude_label and tag.startswith('B-'):\n",
    "                span_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                # Collect I-XXX continuation tags\n",
    "                while i < len(tokens) and tags[i].startswith('I-'):\n",
    "                    span_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "                entity_string = ' '.join(span_tokens)\n",
    "                grouped_strings.add(entity_string)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return grouped_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = collect_all_entity_strings(train_data)\n",
    "dev_tokens = collect_all_entity_strings(dev_data)\n",
    "test_tokens = collect_all_entity_strings(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap loc train:  {'Abu Dhabi', 'Syrien', 'Kuwait', 'Irak', 'Ankara', 'Bush', 'Luxor', 'Erzincan'}\n",
      "overlap loc dev:  {'Oman', 'Bahrain'}\n",
      "overlap loc test:  {'Irak', 'Bagdad'}\n",
      "overlap org train:  {'CBC'}\n",
      "overlap org dev:  {'CBC'}\n",
      "overlap org test:  set()\n",
      "overlap BPER train:  {'Elias', 'Bassam'}\n",
      "overlap BPER dev:  set()\n",
      "overlap BPER test:  set()\n"
     ]
    }
   ],
   "source": [
    "print(\"overlap loc train: \", train_tokens & ME_LOC_tokens)\n",
    "print(\"overlap loc dev: \", dev_tokens & ME_LOC_tokens)\n",
    "print(\"overlap loc test: \", test_tokens & ME_LOC_tokens)\n",
    "\n",
    "print(\"overlap org train: \", train_tokens & ME_ORG_tokens)\n",
    "print(\"overlap org dev: \", dev_tokens & ME_ORG_tokens)\n",
    "print(\"overlap org test: \", test_tokens & ME_ORG_tokens)\n",
    "\n",
    "print(\"overlap BPER train: \", train_tokens & ME_BPER_tokens)\n",
    "print(\"overlap BPER dev: \", dev_tokens & ME_BPER_tokens)\n",
    "print(\"overlap BPER test: \", test_tokens & ME_BPER_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ME_BPER_tokens: 735\n",
      "Updated ME_BPER_tokens: 733\n"
     ]
    }
   ],
   "source": [
    "print(\"before ME_BPER_tokens:\", len(ME_BPER_tokens))\n",
    "updated_ME_BPER = list(ME_BPER_tokens - (train_tokens & ME_BPER_tokens) - (dev_tokens & ME_BPER_tokens) - (test_tokens & ME_BPER_tokens))\n",
    "print(\"Updated ME_BPER_tokens:\", len(updated_ME_BPER))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ME_LOC: 481\n",
      "After ME_LOC: 470\n"
     ]
    }
   ],
   "source": [
    "# Define the overlap set (tokens from train, dev, and test)\n",
    "dataset_spans = train_tokens | dev_tokens | test_tokens  # Combine all three sets (train, dev, test)\n",
    "\n",
    "# Step 3: Remove overlapping entries from ME_LOC\n",
    "filtered_ME_LOC = []\n",
    "for item in ME_LOC:\n",
    "    entity_string = ' '.join(item['tokens'])  # Join tokens into span string\n",
    "    if entity_string not in dataset_spans:    # Keep only if it's NOT overlapping\n",
    "        filtered_ME_LOC.append(item)\n",
    "\n",
    "# Step 4: Optional check\n",
    "print(\"Before ME_LOC:\", len(ME_LOC))\n",
    "print(\"After ME_LOC:\", len(filtered_ME_LOC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ME_ORG: 427\n",
      "After ME_ORG: 426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Remove overlapping entries from ME_LOC\n",
    "filtered_ME_ORG = []\n",
    "for item in ME_ORG:\n",
    "    entity_string = ' '.join(item['tokens'])  # Join tokens into span string\n",
    "    if entity_string not in dataset_spans:    # Keep only if it's NOT overlapping\n",
    "        filtered_ME_ORG.append(item)\n",
    "\n",
    "# Step 4: Optional check\n",
    "print(\"Before ME_ORG:\", len(ME_ORG))\n",
    "print(\"After ME_ORG:\", len(filtered_ME_ORG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ME_LOC = collect_entity_strings(filtered_ME_LOC, target_label_prefix = \"B-LOC\")\n",
    "\n",
    "filtered_ME_ORG = collect_entity_strings(filtered_ME_ORG, target_label_prefix = \"B-ORG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap loc train:  set()\n",
      "overlap loc dev:  set()\n",
      "overlap loc test:  set()\n",
      "overlap org train:  set()\n",
      "overlap org dev:  set()\n",
      "overlap org test:  set()\n",
      "overlap BPER train:  set()\n",
      "overlap BPER dev:  set()\n",
      "overlap BPER test:  set()\n"
     ]
    }
   ],
   "source": [
    "print(\"overlap loc train: \", train_tokens & filtered_ME_LOC)\n",
    "print(\"overlap loc dev: \", dev_tokens & filtered_ME_LOC)\n",
    "print(\"overlap loc test: \", test_tokens & filtered_ME_LOC)\n",
    "\n",
    "print(\"overlap org train: \", train_tokens & filtered_ME_ORG)\n",
    "print(\"overlap org dev: \", dev_tokens & filtered_ME_ORG)\n",
    "print(\"overlap org test: \", test_tokens & filtered_ME_ORG)\n",
    "\n",
    "print(\"overlap BPER train: \", train_tokens & set(updated_ME_BPER))\n",
    "print(\"overlap BPER dev: \", dev_tokens & set(updated_ME_BPER))\n",
    "print(\"overlap BPER test: \", test_tokens & set(updated_ME_BPER))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount):\n",
    "    # First, filter sentences that have at least one non-\"O\" tag\n",
    "    eligible_sentences = [sent for sent in dataset if any(tag != \"O\" for tag in sent[\"ner_tags\"])]\n",
    "\n",
    "    # Select up to sentence_amount randomly from the eligible ones\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "\n",
    "\n",
    "    for sent in selected_sentences:\n",
    "        i = 0\n",
    "\n",
    "        while i<len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                replace = random.choice(ME_BPER)\n",
    "                sent[\"ner_tags\"][i] = replace\n",
    "                ME_BPER.remove(replace)\n",
    "                i+=1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                replace = random.choice(ME_IPER)\n",
    "                sent[\"ner_tags\"][i] = replace\n",
    "                ME_IPER.remove(replace)\n",
    "                i+=1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                same_length_LOC = []\n",
    "                for loc in ME_LOC:\n",
    "                    if len(loc) == span_len:\n",
    "                        same_length_LOC.append(loc)\n",
    "                if same_length_LOC:\n",
    "                    replace = random.choice(same_length_LOC)\n",
    "                    sent[\"tokens\"][span_start:span_start+span_len] = replace\n",
    "                    ME_LOC.remove(replace)\n",
    "            \n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                same_length_ORG = []\n",
    "                for org in ME_ORG:\n",
    "                    if len(org) == span_len:\n",
    "                        same_length_ORG.append(org)\n",
    "                if same_length_ORG:\n",
    "                    replace = random.choice(same_length_ORG)\n",
    "                    sent[\"tokens\"][span_start:span_start+span_len] = replace\n",
    "                    ME_ORG.remove(replace)\n",
    "\n",
    "            else:\n",
    "                i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2143288949.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[75], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dev_data =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dev_data = \n",
    "what = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
