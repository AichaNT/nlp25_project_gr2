{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from ME_BPER import ME_BPER\n",
    "from ME_IPER import extract_last_names\n",
    "from ME_LOC import add_location\n",
    "from ME_ORG import add_organisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading label data from a given column\n",
    "# this is the readNlu function from the provided span_f1 file\n",
    "# minor modifications were made to make it usable with our data. \n",
    "def readNlu(path, target_column = 1): # default to index 1 (thats where DaN+ labels are)\n",
    "    '''\n",
    "    This function reads labeled annotations from a CoNLL-like file.\n",
    "\n",
    "    It parses a file where each line typically represents a single token and its annotations,\n",
    "    separated by tabs. Empty lines denote sentence boundaries. It extracts labels from a specified column\n",
    "    (by default, column index 1), collecting them as a list of label sequences, one per sentence.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the input file.\n",
    "        target_column (int, optional): Index of the column to extract labels from. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: A list where each element is a list of labels (strings) corresponding\n",
    "                         to tokens in a sentence.\n",
    "    '''\n",
    "\n",
    "    annotations = []    # list for storing all the label sequences (one per sentence)\n",
    "    cur_annotation = [] # temp list for labels of the current sentence\n",
    "\n",
    "    # reading through the file line by line\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()                     # remove leading/trailing whitespaces\n",
    "\n",
    "        # empty lines denotes end of sentence\n",
    "        if line == '':\n",
    "            annotations.append(cur_annotation)  # add current annotations to annotations list\n",
    "            cur_annotation = []                 # reset for the next sentence\n",
    "        \n",
    "        # skipping comments (start with \"#\" and no tokens columns)\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            # extract the label from the specified column and add to current sentence\n",
    "            cur_annotation.append(line.split('\\t')[target_column])\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# mapping funciton \n",
    "def mapping(path):\n",
    "    '''\n",
    "    This function generates mappings between labels and their corresponding integer IDs from a labeled dataset.\n",
    "\n",
    "    It reads annotations from a CoNLL-like file using the `readNlu` function,\n",
    "    filters out labels containing substrings like \"part\" or \"deriv\" (case-insensitive),\n",
    "    and creates a bidirectional mapping between the remaining unique labels and integer IDs.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the labeled data file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], Dict[int, str]]:\n",
    "            - label2id: A dictionary mapping each label to a unique integer ID.\n",
    "            - id2label: A reverse dictionary mapping each integer ID back to its label.\n",
    "    '''\n",
    "\n",
    "    # get the data labels\n",
    "    data_labels = readNlu(path) \n",
    "\n",
    "    # create empty set to store unique labels\n",
    "    label_set = set()\n",
    "\n",
    "    for labels in data_labels:\n",
    "        #  filter out any labels that contain 'part' or 'deriv' (case-insensitive)\n",
    "        filtered = [label for label in labels if 'part' not in label.lower() and 'deriv' not in label.lower()]\n",
    "        label_set.update(filtered)\n",
    "\n",
    "    # count of unique filtered labels\n",
    "    num_labels = len(label_set)\n",
    "\n",
    "    # create a dictionary mapping each label to a unique integer ID\n",
    "    label2id = {label: id for id, label in enumerate(label_set)}\n",
    "\n",
    "    # create a dictionary mapping each unique integer ID to a label\n",
    "    id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "# load data function\n",
    "# heavily inspired by the solution from assignment 5\n",
    "def read_tsv_file(path, label2id):\n",
    "    '''\n",
    "    This function reads a TSV file containing tokens and NER labels and converts it into structured data.\n",
    "    It collects the tokens, their original labels, and their corresponding integer IDs (based on the provided `label2id` mapping) for each sentence.\n",
    "    Sentences are separated by empty lines. \n",
    "\n",
    "    Each non-empty line in the file is expected to have at least two tab-separated columns:\n",
    "    - The first column is the token.\n",
    "    - The second column is the corresponding NER label.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the TSV file to read.\n",
    "        label2id (dict): A dictionary mapping NER label strings to their corresponding integer IDs.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries, one per sentence, with keys:\n",
    "            - 'tokens': list of tokens.\n",
    "            - 'ner_tags': list of original NER label strings.\n",
    "            - 'tag_ids': list of integer tag IDs corresponding to the NER labels.\n",
    "    '''\n",
    "\n",
    "    data = []               # final list to hold all sentences as dictionaries\n",
    "    current_words = []      # tokens for the current sentence\n",
    "    current_tags = []       # NER tags for the current sentence\n",
    "    current_tag_ids = []    # corresponding tag IDs for the current sentence\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip() # removes any leading and trailing whitespaces from the line\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#': \n",
    "                continue # skip comments\n",
    "\n",
    "            # splitting at 'tab', as the data is tab separated \n",
    "            tok = line.split('\\t')\n",
    "            \n",
    "            # extract the token (first column)\n",
    "            token = tok[0]\n",
    "\n",
    "            # check if the label is in the provided label2id dictionary\n",
    "            # if it's not, replace the label with 'O'\n",
    "            label = tok[1] if tok[1] in label2id else 'O'\n",
    "\n",
    "            current_words.append(token)\n",
    "            current_tags.append(label)\n",
    "            current_tag_ids.append(label2id[label])\n",
    "        \n",
    "        else: # skip empty lines\n",
    "            if current_words: # if current_words is not empty\n",
    "\n",
    "                # add entry to dict where tokens and ner_tags are keys and the values are lists\n",
    "                data.append({\"tokens\": current_words, \"ner_tags\": current_tags, \"tag_ids\": current_tag_ids})\n",
    "\n",
    "            # start over  \n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "            current_tag_ids = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append({\"tokens\": current_words, \"ner_tags\": current_tags, \"tag_ids\": current_tag_ids})\n",
    "  \n",
    "    return data\n",
    "\n",
    "# extracting tokens to check for overlap in train, dev and test sets\n",
    "def extract_labeled_tokens(dataset, exclude_label = \"O\", include_label_pair=False):\n",
    "    '''\n",
    "    This function extracts tokens from a dataset that have a string label different from `exclude_label`.\n",
    "    Optionally, it can return the (token, label) pairs instead of just tokens.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (List[dict]): The token-tagged dataset.\n",
    "        exclude_label (str): The label to ignore (default is 'O').\n",
    "        include_label_pair (bool): Whether to include the (token, label) pairs in the result (default is False).\n",
    "        \n",
    "    Returns:\n",
    "         Set[str] or Set[Tuple[str, str]]: \n",
    "            - A set of tokens with meaningful (non-O) labels if `include_label_pair` is False.\n",
    "            - A set of (token, label) pairs if `include_label_pair` is True.\n",
    "    '''\n",
    "\n",
    "    # create empty set to store the unique tokens\n",
    "    labeled_tokens = set()\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        # iterate over each token and its corresponding tag ID\n",
    "        for token, label in zip(sentence[\"tokens\"], sentence[\"ner_tags\"]):\n",
    "            if label != exclude_label:                      # check if the tag is not the excluded one\n",
    "                if include_label_pair:\n",
    "                    labeled_tokens.add((token, label))      # add (token, label) pair if the flag is True\n",
    "                else:\n",
    "                    labeled_tokens.add(token)               # add just the token if the flag is False\n",
    "    \n",
    "    return labeled_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_aug_sources/the-middle-east-cities.csv\", sep = \";\", skiprows = 1)\n",
    "unique_city_da = df[\"city_da\"].drop_duplicates()\n",
    "ME_LOC = [add_location(loc) for loc in unique_city_da]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_aug_sources/middle_eastern_organisations.csv\", sep = \";\", skiprows = 1)\n",
    "unique_orgs = df[\"org\"].drop_duplicates()\n",
    "ME_ORG = [add_organisation(org) for org in unique_orgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_entity_strings(data, target_label_prefix):\n",
    "    \"\"\"\n",
    "    Collects labeled spans as strings from the dataset. Multi-token spans are joined with spaces.\n",
    "    \n",
    "    Args:\n",
    "        data (List[Dict]): Dataset containing 'tokens' and 'tags' for each sentence.\n",
    "        target_label_prefix (str): Label prefix to filter for (e.g., 'B-LOC', 'B-ORG').\n",
    "        \n",
    "    Returns:\n",
    "        Set[str]: A set of labeled token strings (e.g., {'Beirut', 'Al Mawsil al Jadidah'})\n",
    "    \"\"\"\n",
    "    grouped_strings = set()\n",
    "\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        tags = item['tags']\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tag = tags[i]\n",
    "\n",
    "            if tag.startswith(target_label_prefix):\n",
    "                span_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                while i < len(tokens) and tags[i].startswith('I'):\n",
    "                    span_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "                # Join tokens into a single string and add to the set\n",
    "                entity_string = ' '.join(span_tokens)\n",
    "                grouped_strings.add(entity_string)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return grouped_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Robat Karim', 'Kayseri', 'Jirja', 'Beylikduezue', 'Esenler', 'Ad Diwaniyah', 'Torbat-e Heydariyeh', 'Borazjan', 'Rize', 'Ras Beirut', 'Zagazig', 'Al Fashn', 'Wadi as Sir', 'Khobar', 'Trabzon', 'Marivan', 'Al Fahahil', 'Ahar', 'Azadshahr', 'Ödemiş', 'Birjand', 'Najaf', 'Sidon', 'Bagdad', 'Ajman', 'Sakakah', 'Habbouch', 'Gemlik', 'Kerman', 'Sohar', 'Lüleburgaz', 'As-Salt', 'Kadirli', 'Khomeyn', 'Bam', 'Polatlı', 'Takestan', 'Erzincan', 'Ashmun', 'Bahrain', 'Jablah', 'Ordu', 'Ath Thawrah', 'Forenede Arabiske Emirater', 'Batikent', 'Zahle', 'Batman', 'Saqqez', 'Tyrkiet', 'Bani Suwayf', 'Nusaybin', 'Seeb', 'Çorum', 'Aswan', 'Daraa', 'Sirjan', 'Diyarbakir', 'Edfu', 'Al Matariyah', 'Akşehir', 'Nurabad', 'Zanjan', 'Fatsa', 'Teheran', 'Kermanshah', 'Hebron', 'As Salamiyah', 'Adana', 'Jizan', 'Dogonbadan', 'Sharjah', 'Al Khankah', 'Abu Kabir', 'Umm Qasr', 'Dikirnis', 'Jamjamal', 'Denizli', 'Borujerd', 'Osmaniye', 'Silvan', 'Adapazari', 'Sur', 'Sanandaj', 'Nizwa', 'Karabağlar', 'Bismil', 'Erciş', 'Samsun', 'Al Mawsil al Jadidah', 'Kafr ad Dawwar', 'Kirikkale', 'Zarqa', 'Manfalut', 'Kirkuk', 'Luxor', 'Al-Hudaydah', 'Bandar Abbas', 'Medina', 'Deir ez-Zor', 'Al Fujayrah', 'Darayya', 'Shahre Jadide Andisheh', 'Shibin al Kawm', 'Mosul', 'Adiyaman', 'Kuwait', 'Disuq', 'Karaj', 'Al Basrah al Qadimah', 'Mashhad', 'Atasehir', 'Tokat', 'Ajlun', 'Douma', 'Patnos', 'Aydin', 'Erbil', 'Halwan', 'Herzliya', 'Yanbu', 'Hawalli', 'Kuşadası', 'Iğdır', 'Al Bab', 'Sultanbeyli', 'Libanon', 'Bolu', 'Piranshahr', 'Al Faw', 'Samalut', 'Sohag', 'Erzurum', 'Syrien', 'Al Manzalah', 'Bilbeis', 'Ash Shafa', 'Iran', 'Soma', 'Mallawi', 'Tatvan', 'Parsabad', 'Ceyhan', 'Tabriz', 'Saveh', 'Al Jammaliyah', 'Şişli', 'Khamis Mushait', 'Fuwwah', \"Ta'if\", 'Østjerusalem', 'Barka', 'Isparta', 'Al Minya', 'Niğde', 'Ash Shatrah', 'Langarud', 'As Safirah', 'Al-Hasakah', 'Al Kharijah', 'Bilqas', 'Kashmar', 'Al Mahallah al Kubra', 'Elazig', 'Alvand', 'Russeifa', 'Turgutlu', 'Ar Ramtha', 'Karaman', 'Zonguldak', 'Sayyan', 'Al Jubayl', 'Ilam', 'Bushehr', 'Ardabil', 'Maltepe', 'Babol', 'Mahabad', 'Cizre', 'Izmir', 'Izmit', 'Taiz', 'Saudi Arabien', 'Iranshahr', 'Ras al-Khaimah', 'Firuzabad', 'Bukan', 'Isfahan', 'As Salimiyah', 'Beirut', 'Al Fallujah', 'Akbarabad', 'Manavgat', 'Kızıltepe', 'Khorramabad', 'Rasht', 'Mardin', 'Samarra', 'Rosetta', 'Kutahya', 'Bishah', 'Körfez', 'Munuf', 'Damghan', 'Gonbad-e Kavus', 'Housh Eissa', 'Dammam', 'Ümraniye', 'Qurayyat', 'Başakşehir', 'Elbistan', 'Al Ain', 'Yazd', 'Qina', 'Giresun', 'Dehdasht', 'Ereğli', 'Abhar', 'Shiraz', 'Istanbul', 'Midyat', 'Zeytinburnu', 'Najafabad', 'Ismailia', 'Idlib', 'Ardeşen', 'Bingöl', 'Damaskus', 'Hamadan', 'Ar Rass', 'Konya', 'Muscat', 'Abu Ghurayb', 'Riyadh', 'Çerkezköy', 'Abu Dhabi', 'Al-Arish', 'As Suwayq', 'Turhal', 'Hamah', 'Van', 'Sinah', 'Palæstina', 'Hakkari', 'Tarsus', 'Siirt', 'Minab', 'Idku', 'Khoy', 'Port Said', 'Najran', 'Uşak', 'Kufa', 'Aden', 'Al Qusiyah', 'Baneh', 'Latakia', 'Salalah', 'Al Hayy', 'Sorgun', 'Çanakkale', 'Tripoli', 'Al Ahmadi', 'Hafar Al-Batin', 'Yozgat', 'Qorveh', 'Dahuk', 'Bahçelievler', 'Tabuk', 'Mersin', 'Esenyurt', 'Faqus', 'Bandırma', 'Mersa Matruh', 'Yüksekova', 'Basra', 'Al Hufuf', 'Mustafakemalpaşa', 'Mukalla', 'Ünye', 'Aleppo', 'Qarchak', 'Nizip', 'Kırşehir', 'Tartus', 'Arnavutköy', 'Silifke', 'Neyshabur', 'Corlu', 'Nevşehir', 'Tahta', 'Giza', 'Khorramshahr', \"Al 'Amarah\", 'Jabalya', 'Sabah as Salim', 'Semnan', 'Aqaba', 'Arar', 'Naqadeh', 'Marand', 'Sayhat', 'Amol', 'Amman', 'Damanhur', 'Aksaray', 'Zaxo', 'Dubai', 'Sanaá', 'Darab', 'Ar Rifa', 'Kairo', 'Salihli', 'Tekirdağ', 'Shahr-e Kord', 'Al Bahah', 'İnegöl', 'Az Zubayr', 'Sultangazi', 'Alexandria', 'Sultanah', 'Dhahran', 'Masjed Soleyman', 'Khanjarah', 'Buraydah', 'Şanlıurfa', 'Al Mansurah', 'Antakya', 'Jordan', 'Nabatiye et Tahta', 'Amran', 'Miandoab', 'Al Kharj', 'Hurghada', 'Ibb', 'Al Mubarraz', 'Karbala', 'Talkha', 'Manama', 'Behbahan', 'Ar Rayyan', 'Fasa', 'Zahedan', 'Siverek', 'Belek', 'Üsküdarr', 'Kahriz', 'Alanya', 'Zabol', 'Khash', 'Bojnurd', 'Gaziantep', 'Al Hawamidiyah', 'Ağrı', 'Kahta', 'Toukh', 'Asyut', 'Sulaymaniyah', 'Rustaq', 'Eskisehir', 'Saham', 'Rafah', 'Salmas', 'Akhmim', 'Gaza', 'Qazvin', 'Zefta', 'Damietta', 'Ramadi', 'Rukban', 'Muratpasa', 'Malayer', 'Jounieh', 'Awsim', 'Ahlat', 'Çankaya', 'Al Qatif', 'Viranşehir', 'Yalova', 'New Cairo', 'Isna', 'Dhamar', 'Afyonkarahisar', 'Doha', 'Kahramanmaraş', 'Büyükçekmece', 'Abnub', 'Manisa', 'Qalyub', 'Malatya', 'Çubuk', 'Iskenderun', 'Madaba', 'Dar Kulayb', \"A'zaz\", 'Tarut', 'Gebze', 'Shirvan', 'Nazarabad', 'Banha', 'Arak', 'Bağcılar', 'Silopi', 'Chalus', 'Kafr az Zayyat', 'Anzal-e Jonubi', 'Gorgan', 'Al Harithah', 'Sabzevar', 'Balikesir', 'Sari', 'Abadan', 'Kazerun', 'Bandar-e Anzali', 'Al Muharraq', 'Yasuj', 'Edirne', 'Al Fayyum', 'Jidda', 'Ibri', 'Tyre', 'Shahrud', 'Sancaktepe', 'Qom', 'Kozan', 'Behshahr', 'Khan Yunis', 'Manbij', 'Abu Tij', 'Dayrut', 'Antalya', 'Suez', 'Merkezefendi', 'Homs', 'Aligudarz', 'Muş', 'Mekka', 'Kilis', 'Karabük', 'Nazilli', 'Kars', 'Söke', 'Abha', 'Al Farwaniyah', 'Orumiyeh', 'Doğubayazıt', 'Al Kut', 'Bawshar', 'As Samawah', 'Varamin', \"Ha'il\", 'Khomeyni Shahr', 'Ar Raqqah', 'Shushtar', 'Al Hillah', 'Kafr ash Shaykh', 'Quchan', 'Al Buraymi', 'Khalis', 'Oman', 'Bonab', 'Akhisar', 'Bursa', 'Burdur', 'Ayvalık', 'Kuhdasht', 'Bafra', 'Kastamonu', 'Irak', 'Rafsanjan', 'Bush', 'Ankara', 'Nahavand', 'Nablus', 'Sivas', 'Ahvaz', 'Tanda', 'Nasiriyah', 'Irbid', 'Baqubah', 'Egypten'}\n",
      "480\n",
      "481\n",
      "\n",
      "\n",
      "{'BMMI', 'Starworld', 'Altibbi', 'Al-Anbaa', 'Dubai Electricity and Water Authority', 'Henkel GCC', 'Al Manar', 'Cinescape', 'Al Kuwaitiya', 'Al-Arab Al-Yawm', 'Echorouk', 'Elves', 'Yallacompare', 'Ahlibank', 'Abu Dhabi Islamic Bank', 'Muscat Stock Exchange', 'SNRT', 'Aiguebelle', 'ADNOC', 'GEMS Education', 'Qatar Fuel', 'Kingdom Holding Company', 'Adaraweesh', 'Commercial Bank of Dubai', 'Al-Mashriq', 'Saudi National Bank', 'BiscoMisr', 'Baladna', 'Al Rayaam', 'Fajr Capital', 'NBB Group', 'Americana Restaurants', 'Saudi Aramco Base Oil Company', 'Bank Of Africa', 'Abjjad', 'Doha Bank', 'Daraty', 'AL24 News', 'Al Ayam', 'Bahri', 'al-Fajr al-Jadid', 'Alinma Bank', 'Ma’aden', 'Fetchr', 'Al-Waie', 'Bayanat', 'Xenel', 'Emirates Islamic', 'Shihan', 'SOMED', 'The Syrian American Medical Society', 'Ad-Dustour', 'Kuwait Projects Company', 'AKKASA', 'NMDC Group', 'Al Raya', 'The Ghassan Aboud Group', 'Shorooq Partners', 'stc Group', 'Electroplanet', 'Asdaa Burson Marsteller', 'Arryadia', 'Jumeirah Group', 'Thumbay Group', 'Oilibya', 'Laraki', 'Al-Thawra', 'Ferrimaroc', 'Al-Bilad', 'National Bank of Fujairah', 'Savola Group', 'Gulf Madhyamam', 'Chefaa', 'Epilert', 'GoEjaza', 'Al-Muhaidib', 'Nakilat', 'Mo’men', 'Solfeh', 'Lamsa', 'DEWA', 'Siera', 'Alwasat', 'Commercial International Bank', 'Al-Tijari', 'Ooredoo', 'Arab Fund for Economic and Social Development', 'G42', 'Assabeel', 'Nouvelair', 'Khartoum', 'AvidBeam', 'Somaca', 'Akhbar Al Khaleej', 'Mubadala', 'al-Bayan', 'solutions by stc', 'RasGas', 'Amanat Holdings', 'QNB Group', 'Al Shorouk', 'Lucidya', 'Koutoubia', 'Tishreen', 'The Noor Dubai Foundation', 'Omantel', 'Bank AlJazira', 'Emaar Development', 'Investcorp', 'Banagas', 'Kayhan Al Arabi', 'Saudi Investment Bank', 'National Bank of Ras Al Khaimah', 'First Abu Dhabi Bank', 'Al-Kifah al-Arabi', 'Dubai Airports Company', 'ZenHR Solutions', 'Roya TV', 'El Heddaf', 'Naseej', 'One', 'Jumeirah Hotels', 'Nilesat', 'Emirates NBD', 'JoSat', 'Al Watan', 'Swvl', 'PSLab', 'eSpace', 'Bakdash', 'Mazazikh', 'Afriquia', 'Okaz', 'Nehmeh', 'Bimo', 'al-Alam', 'ArabiaWeather', 'Zaytouna TV', 'Nareva', 'Arasco', 'Saudi Awwal Bank', 'Al-Mamlaka TV', 'al-Jarida al-Maghribia', 'BMCI', 'National Bank of Kuwait', 'Echorouk Group', 'Al Yaum', 'Kahramaa', 'ACWA Power', 'Qatar Islamic Bank', 'Souqalmal', 'DMC', 'TAQA Group', 'Sonasid', 'Danube Group', 'al-Jamahiriyah', 'Eco-Médias', 'Almarai', 'Nagham', 'Al-Intiqad', 'Al Sharq', 'Attijariwafa bank group', 'Careem', 'Al-Nahar', 'Virtuzone', 'Istiqlal Party', 'ONTV', 'Siger', 'Shamsina', 'Sonatrach', 'Ashabiba', 'Daily Sabah', 'Sharjah Islamic Bank', 'Monoprix', 'Sahara International Petrochemical Company', 'Elm', '2M', 'Djezzy', 'Etisalat', 'Agility', 'Saudi Electricity Company', 'al-Furat', 'Balad Party', 'Jawan', 'ibTECHar Digital Solutions', 'Dubai Islamic Bank', 'Al Liwaa', 'Libyana', 'National Shipping Company of Saudi Arabia', 'Tunisavia', 'Al Nabooda Automobiles', 'Cerebras', 'al-Balad', 'Industries Qatar', 'Talabat', 'The International Islamic Charitable Organization', 'Damac', 'LDC', 'Patchi', 'Kuwait Oil Company', 'Alshaya', 'Aïcha', 'Adwya', 'LuLu Group', 'Ooredoo Group', 'Mauritel', 'Arab Socialist Union Party of Syria', 'Iqraa', 'Ras Al Khaimah Economic Zone', 'Saidal', 'Emirates Airline', 'Marjane', 'Wallyscar', 'Empower', 'Mashreq', 'ANB', 'Deyaar', 'Hawaï', 'Omdurman', 'Ad Diyar', 'Télé Liban', 'Syriatel', 'alrajhi bank', 'Wikaya', \"Arab Socialist Ba'ath Party\", 'Bank Muscat', 'K24', 'Al-Horria', 'PubliTools', 'Air Arabia', 'Dubai Investments', 'Qatar International Islamic Bank', 'Burgan Bank Group', 'Al-Massira', 'Banque Saudi Fransi', 'Kharabeesh', 'Kuwait Finance House', 'Al Rayyan', 'Nadec', 'National Covenant Party', 'Mazzika', 'Athaqafia', 'DenizBank', 'Inwi', 'Bank Albilad', 'Alrifai', 'Akdital', 'Saudi Arabian Mining Company', 'The Jordan River Foundation', 'Commercial Bank', 'Al Quds Association', 'Kudu', 'ViaVii', 'Hawacom', 'Tunisna', 'Mathaqi', 'Dr. Sulaiman Al Habib Medical Services Group', 'Al Ahli Bank of Kuwait', 'Dussur', 'Gulfsat', 'Al Rai', 'Palestinian Liberation Front', 'Al Anbaa', 'Kharafi Group', 'TGCC', 'Mawdoo3', '3ayez', 'Al Tayer Group', 'Unifonic', 'Kezad', 'Mekameleen TV', 'Al-Ahram', 'al-Zahf Al-Akhdar', 'Derq', 'Saudia', 'ADCB Group', 'Saudi Aramco', 'Wanasah', 'AD Ports Group', 'El Mehwar', 'Al-Wehda', 'Akhbar Nouakchott', 'Al-Ouruba', 'Al Massar', 'MarsaMaroc', 'Al Jumhuriya', 'Al Eqtisadiah', 'OTV', 'Managem', 'Dukhan Bank', 'Malaeb', 'Al Karmil', 'NBN', 'Marafiq', 'Asmidal', 'Filasteen al-Muslimah', 'Qaym', 'Sopriam', 'The Qatar Fund for Development', 'The Arab Medical Union', 'Tamatem', 'Evertek', 'Emirates Integrated Telecommunications Company', 'Assadissa', 'Borouge', 'Riyad Bank', 'SNVI', 'Tabbah', 'Aluminium Bahrain', 'Vermeg', 'Masraf Al Rayan', 'Jordan Phosphate Mines Company', 'Cima', 'Al-Ittihad', 'RASCO', 'Aumet', 'Egyptalum', 'Gulf News', 'PureHealth Holding', 'QEWC', 'HalalaH', 'Ajeer', 'Boubyan Bank', 'Inagrab', 'ONCF', 'Bahrain Islamic Bank', 'Arab Bank', 'Al Jamahir', 'Emsteel', 'Etihad Etisalat Company', 'Qatar Charity', 'International Holding Company', 'Marina Home', 'Polisario Front', 'The Palestinian Medical Relief Society', 'Kitea', 'The UAE’s Zayed Giving Initiative', 'Dabchy', 'Mondair', 'EgyptAir', 'The Khalifa Foundation', 'Arab Democratic Nasserist Party', 'The Company for Cooperative Insurance', 'Bupa Arabia', 'KTV2', 'Al-Kalima', 'Aldar Properties', 'Spinneys', 'GIB Capital', 'Comarit', 'Batelco', 'QNB ALAHLI', 'Alpha Dhabi Holding', 'al-Watan', 'Aramco', 'Emaar Properties', 'Emirates Red Crescent', 'Al-Ahd Ul’Jadid', 'Zain KSA', 'Al Amal', 'Sarwa', 'SAMI', 'CBC', 'Tarjama', 'ADES Holding', 'Flynas', 'Apparel Group', 'Jet4you', 'Colorado', 'Gulf Bank', 'OSN', 'Al Mada', 'Massaya', 'Al Aoula', 'SABIC Agri-Nutrients Company', 'Naftal', 'KIPCO', 'Mrsool', 'Qatar Insurance Company', 'Bank ABC', 'Sawani', 'Jamalon', 'ADNOC Gas', '360VUZ', 'Noon', 'Al Jazeera', 'Rezayat', 'al-Qabas', 'BCP Group', 'SGTM', 'Zain Group', 'Fertiglobe', '218TV', 'Argaam', 'Biougnach', 'Nabd', 'Khaleej Times', 'Al-Mustaqbal', 'al-Hayat al-Jadida', 'Sonalgaz', 'Islamic Relief Worldwide', 'Wasla', 'Du', 'BulkWhiz', 'Arabot', 'Maroc Telecom', 'beIN SPORTS', 'Al Anbat', 'MBC', 'Shawarmer', 'Torath', 'Amal Glass', 'Nasr', 'Attajdid', 'Sela', 'Al Madina', 'arab national bank', 'El Hiwar El Tounousi', 'Cevital', 'Dnata', 'Aster DM Healthcare', 'e&', 'Popular Movement in Iraq', 'Al Yah Satellite Communications', 'Bayane al-Yaoume', 'Mobily', 'Asiacell'}\n",
      "427\n",
      "427\n"
     ]
    }
   ],
   "source": [
    "# overlap between train, dev, test and MENAPT NEs\n",
    "ME_LOC_tokens = collect_entity_strings(ME_LOC, target_label_prefix = \"B-LOC\")\n",
    "\n",
    "ME_ORG_tokens = collect_entity_strings(ME_ORG, target_label_prefix = \"B-ORG\")\n",
    "\n",
    "print(ME_LOC_tokens)\n",
    "print(len(ME_LOC_tokens))\n",
    "print(len(ME_LOC))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(ME_ORG_tokens)\n",
    "print(len(ME_ORG_tokens))\n",
    "print(len(ME_ORG))\n",
    "\n",
    "ME_BPER_tokens = set(ME_BPER)\n",
    "#ME_IPER_tokens = set(ME_IPER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_train = \"../data/da_news_train.tsv\"\n",
    "path_dev = \"../data/da_news_dev.tsv\"\n",
    "path_test = \"../data/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_entity_strings(data, exclude_label=\"O\"):\n",
    "    \"\"\"\n",
    "    Collects all labeled (non-\"O\") entity spans as strings from the dataset.\n",
    "    Multi-token spans are joined with spaces.\n",
    "\n",
    "    Args:\n",
    "        data (List[Dict]): Dataset with 'tokens' and 'tags' per sentence.\n",
    "        exclude_label (str): Label to ignore (default is 'O').\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: Set of labeled entity strings (e.g., {'Beirut', 'Al Mawsil al Jadidah'})\n",
    "    \"\"\"\n",
    "    grouped_strings = set()\n",
    "\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        tags = item['ner_tags']\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            tag = tags[i]\n",
    "\n",
    "            if tag != exclude_label and tag.startswith('B-'):\n",
    "                span_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                # Collect I-XXX continuation tags\n",
    "                while i < len(tokens) and tags[i].startswith('I-'):\n",
    "                    span_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "                entity_string = ' '.join(span_tokens)\n",
    "                grouped_strings.add(entity_string)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return grouped_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = collect_all_entity_strings(train_data)\n",
    "dev_tokens = collect_all_entity_strings(dev_data)\n",
    "test_tokens = collect_all_entity_strings(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap loc train:  {'Syrien', 'Kuwait', 'Erzincan', 'Abu Dhabi', 'Irak', 'Luxor', 'Bush', 'Ankara'}\n",
      "overlap loc dev:  {'Bahrain', 'Oman'}\n",
      "overlap loc test:  {'Irak', 'Bagdad'}\n"
     ]
    }
   ],
   "source": [
    "print(\"overlap loc train: \", train_tokens & ME_LOC_tokens)\n",
    "print(\"overlap loc dev: \", dev_tokens & ME_LOC_tokens)\n",
    "print(\"overlap loc test: \", test_tokens & ME_LOC_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap org train:  {'CBC'}\n",
      "overlap org dev:  {'CBC'}\n",
      "overlap org test:  set()\n"
     ]
    }
   ],
   "source": [
    "print(\"overlap org train: \", train_tokens & ME_ORG_tokens)\n",
    "print(\"overlap org dev: \", dev_tokens & ME_ORG_tokens)\n",
    "print(\"overlap org test: \", test_tokens & ME_ORG_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap BPER train:  {'Bassam', 'K', 'Elias', 'S'}\n",
      "overlap BPER dev:  set()\n",
      "overlap BPER test:  {'Z', 'K'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"overlap BPER train: \", train_tokens & ME_BPER_tokens)\n",
    "print(\"overlap BPER dev: \", dev_tokens & ME_BPER_tokens)\n",
    "print(\"overlap BPER test: \", test_tokens & ME_BPER_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug_replace(dataset, sentence_amount):\n",
    "    # First, filter sentences that have at least one non-\"O\" tag\n",
    "    eligible_sentences = [sent for sent in dataset if any(tag != \"O\" for tag in sent[\"ner_tags\"])]\n",
    "\n",
    "    # Select up to sentence_amount randomly from the eligible ones\n",
    "    selected_sentences = random.sample(eligible_sentences, min(sentence_amount, len(eligible_sentences)))\n",
    "\n",
    "\n",
    "    for sent in selected_sentences:\n",
    "        i = 0\n",
    "\n",
    "        while i<len(sent[\"tokens\"]):\n",
    "            tag = sent[\"ner_tags\"][i]\n",
    "\n",
    "            if tag == 'B-PER':\n",
    "                replace = random.choice(ME_BPER)\n",
    "                sent[\"ner_tags\"][i] = replace\n",
    "                ME_BPER.remove(replace)\n",
    "                i+=1\n",
    "\n",
    "            elif tag == 'I-PER':\n",
    "                replace = random.choice(ME_IPER)\n",
    "                sent[\"ner_tags\"][i] = replace\n",
    "                ME_IPER.remove(replace)\n",
    "                i+=1\n",
    "\n",
    "            elif tag == 'B-LOC':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-LOC\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                same_length_LOC = []\n",
    "                for loc in ME_LOC:\n",
    "                    if len(loc) == span_len:\n",
    "                        same_length_LOC.append(loc)\n",
    "                if same_length_LOC:\n",
    "                    replace = random.choice(same_length_LOC)\n",
    "                    sent[\"tokens\"][span_start:span_start+span_len] = replace\n",
    "                    ME_LOC.remove(replace)\n",
    "            \n",
    "            elif tag == 'B-ORG':\n",
    "                span_start = i\n",
    "                span_len = 1\n",
    "                i += 1\n",
    "                while i < len(sent[\"ner_tags\"]) and sent[\"ner_tags\"][i] == \"I-ORG\":\n",
    "                    span_len += 1\n",
    "                    i += 1\n",
    "\n",
    "                same_length_ORG = []\n",
    "                for org in ME_ORG:\n",
    "                    if len(org) == span_len:\n",
    "                        same_length_ORG.append(org)\n",
    "                if same_length_ORG:\n",
    "                    replace = random.choice(same_length_ORG)\n",
    "                    sent[\"tokens\"][span_start:span_start+span_len] = replace\n",
    "                    ME_ORG.remove(replace)\n",
    "\n",
    "            else:\n",
    "                i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
