{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b878588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import mapping, read_tsv_file, extract_labeled_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d59c4a",
   "metadata": {},
   "source": [
    "# Reading in the data and Looking at overlapping tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78521543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data \n",
    "# path to the data files\n",
    "path_train = \"../data/da_news_train.tsv\"\n",
    "path_dev = \"../data/da_news_dev.tsv\"\n",
    "path_test = \"../data/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ca32c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 4383\n",
      "dev size: 564\n",
      "test size: 565\n",
      "total dataset size: 5512\n"
     ]
    }
   ],
   "source": [
    "print(\"train size:\", len(train_data))\n",
    "print(\"dev size:\", len(dev_data))\n",
    "print(\"test size:\", len(test_data))\n",
    "print(\"total dataset size:\", len(train_data) + len(dev_data) + len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57fb8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens with non-\"O\" labels from each split\n",
    "train_tokens = extract_labeled_tokens(train_data)\n",
    "dev_tokens = extract_labeled_tokens(dev_data)\n",
    "test_tokens = extract_labeled_tokens(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf9e88be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in train: 2635\n",
      "Unique tokens in dev: 470\n",
      "Unique tokens in test: 513\n"
     ]
    }
   ],
   "source": [
    "# print out the number of tokens in each split\n",
    "print(f\"Unique tokens in train: {len(train_tokens)}\")\n",
    "print(f\"Unique tokens in dev: {len(dev_tokens)}\")\n",
    "print(f\"Unique tokens in test: {len(test_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe6abf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique overlapping tokens 405\n"
     ]
    }
   ],
   "source": [
    "# overlap between datasets\n",
    "train_dev_overlap = train_tokens & dev_tokens\n",
    "dev_test_overlap = dev_tokens & test_tokens\n",
    "train_test_overlap = train_tokens & test_tokens\n",
    "\n",
    "# union of all overlapping tokens\n",
    "all_tokens_overlap = train_dev_overlap | dev_test_overlap | train_test_overlap\n",
    "\n",
    "print('Number of unique overlapping tokens', len(all_tokens_overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f5c06",
   "metadata": {},
   "source": [
    "# Make one great dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca257cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5512"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = train_data + dev_data + test_data\n",
    "\n",
    "len(total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd22e79",
   "metadata": {},
   "source": [
    "### Step 1: Extract entities from each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed13880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tokens, tags):\n",
    "    entities = set()\n",
    "    entity = []\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if entity:\n",
    "                entities.add(\" \".join(entity))\n",
    "                entity = []\n",
    "            entity = [token]\n",
    "        elif tag.startswith(\"I-\") and entity:\n",
    "            entity.append(token)\n",
    "        else:\n",
    "            if entity:\n",
    "                entities.add(\" \".join(entity))\n",
    "                entity = []\n",
    "    if entity:\n",
    "        entities.add(\" \".join(entity))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764143d",
   "metadata": {},
   "source": [
    "### Step 2: Build mapping from sentence to entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c35d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "entity_to_sents = defaultdict(list)\n",
    "\n",
    "for idx, sentence in enumerate(total_data):\n",
    "    entities = extract_entities(sentence[\"tokens\"], sentence[\"ner_tags\"])\n",
    "    for ent in entities:\n",
    "        entity_to_sents[ent].append(idx)  # store sentence indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f841bab",
   "metadata": {},
   "source": [
    "### Step 3: Shuffle and split entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54d27a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "entities = list(entity_to_sents.keys())\n",
    "random.shuffle(entities)\n",
    "\n",
    "train_cutoff = int(0.7 * len(entities))\n",
    "dev_cutoff = int(0.8 * len(entities))\n",
    "\n",
    "train_entities = set(entities[:train_cutoff])\n",
    "dev_entities = set(entities[train_cutoff:dev_cutoff])\n",
    "test_entities = set(entities[dev_cutoff:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb983e",
   "metadata": {},
   "source": [
    "### Step 4: Assign sentences to splits based on their entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6c11206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_indices(entity_set):\n",
    "    sent_ids = set()\n",
    "    for ent in entity_set:\n",
    "        sent_ids.update(entity_to_sents[ent])\n",
    "    return sent_ids\n",
    "\n",
    "train_ids = get_sent_indices(train_entities)\n",
    "dev_ids = get_sent_indices(dev_entities) - train_ids\n",
    "test_ids = get_sent_indices(test_entities) - train_ids - dev_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93899ee3",
   "metadata": {},
   "source": [
    "### Step 5: Build final splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "08005a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [total_data[i] for i in train_ids]\n",
    "dev_data = [total_data[i] for i in dev_ids]\n",
    "test_data = [total_data[i] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae15b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1843\n",
      "139\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(dev_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f1871d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['På', 'fredag', 'har', 'SID', 'inviteret', 'til', 'reception', 'i', 'SID-huset', 'i', 'anledning', 'af', 'at', 'formanden', 'Kjeld', 'Christensen', 'går', 'ind', 'i', 'de', 'glade', 'tressere', '.']\n",
      "['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 6, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]['tokens'])\n",
    "print(train_data[0]['ner_tags'])\n",
    "print(train_data[0]['tag_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d24b3",
   "metadata": {},
   "source": [
    "### Writing to tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "705a9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tsv_file(data, path):\n",
    "    '''\n",
    "    Writes a list of sentence dictionaries (with 'tokens' and 'ner_tags') to a TSV file.\n",
    "    Each token-label pair is written on its own line, separated by a tab.\n",
    "    Sentences are separated by empty lines.\n",
    "\n",
    "    Parameters:\n",
    "        data (List[dict]): List of sentence dictionaries.\n",
    "        path (str): Path to write the TSV file to.\n",
    "    '''\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in data:\n",
    "            tokens = sentence['tokens']\n",
    "            ner_tags = sentence['ner_tags']\n",
    "            for token, tag in zip(tokens, ner_tags):\n",
    "                f.write(f\"{token}\\t{tag}\\n\")\n",
    "            f.write(\"\\n\")  # sentence separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b23132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tsv_file(train_data, '../new_data/new_da_news_train.tsv')\n",
    "write_tsv_file(dev_data, '../new_data/new_da_news_dev.tsv')\n",
    "write_tsv_file(test_data, '../new_data/new_da_news_test.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
