{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06cb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.load_data import mapping, extract_labeled_tokens, read_tsv_file, write_tsv_file\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "random.seed(20) # set seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2fd6d",
   "metadata": {},
   "source": [
    "## Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b7cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_news_train = \"../data/da_news/da_news_train.tsv\"\n",
    "path_news_dev = \"../data/da_news/da_news_dev.tsv\"\n",
    "path_news_test = \"../data/da_news/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_news_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data_news = read_tsv_file(path_news_train, label2id)\n",
    "dev_data_news = read_tsv_file(path_news_dev, label2id)\n",
    "test_data_news = read_tsv_file(path_news_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd36e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 4383\n",
      "dev size: 564\n",
      "test size: 565\n",
      "total dataset size: 5512\n"
     ]
    }
   ],
   "source": [
    "# dataset sizes\n",
    "print(\"train size:\", len(train_data_news))\n",
    "print(\"dev size:\", len(dev_data_news))\n",
    "print(\"test size:\", len(test_data_news))\n",
    "print(\"total dataset size:\", len(train_data_news) + len(dev_data_news) + len(test_data_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ffd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate datasets\n",
    "total_data = train_data_news + dev_data_news + test_data_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f218abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraxt unique entities\n",
    "total_entities = extract_labeled_tokens(total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97ac14",
   "metadata": {},
   "source": [
    "## Build mapping from entity to sentence and sentence to entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4144de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict with entities as keys and lists of sentence IDs as values\n",
    "entity_to_sents = defaultdict(set)\n",
    "sent_to_entities = defaultdict(set) # also creating mapping from sentence ID to entity\n",
    "\n",
    "for sent_id, sent in enumerate(total_data):\n",
    "\n",
    "    for tok_id, ent in enumerate(sent[\"tokens\"]):\n",
    "\n",
    "        if ent in total_entities and sent['ner_tags'][tok_id] != 'O':\n",
    "\n",
    "            entity_to_sents[ent].add(sent_id)\n",
    "\n",
    "            sent_to_entities[sent_id].add(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89defe",
   "metadata": {},
   "source": [
    "## Group sentences by overlapping entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ecab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group sentences by shared entities\n",
    "\n",
    "visited = set()\n",
    "sentence_groups = []\n",
    "\n",
    "for sent_id in sent_to_entities:\n",
    "\n",
    "    if sent_id in visited:\n",
    "        continue\n",
    "\n",
    "    group, queue = set(), [sent_id]\n",
    "\n",
    "    while queue:\n",
    "\n",
    "        current = queue.pop()\n",
    "\n",
    "        if current in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(current)\n",
    "        group.add(current)\n",
    "\n",
    "        for entity in sent_to_entities[current]:\n",
    "\n",
    "            queue.extend(entity_to_sents[entity])\n",
    "\n",
    "    sentence_groups.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7abb0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split groups by total sentence count\n",
    "\n",
    "random.shuffle(sentence_groups)\n",
    "\n",
    "train_group, dev_group, test_group, count = [], [], [], 0\n",
    "total = sum(len(g) for g in sentence_groups)\n",
    "train_cutoff, dev_cutoff = int(total * 0.7), int(total * 0.85)\n",
    "\n",
    "for group in sentence_groups:\n",
    "\n",
    "    if count < train_cutoff:\n",
    "        train_group += group\n",
    "        \n",
    "    elif count < dev_cutoff:\n",
    "        dev_group += group\n",
    "\n",
    "    else:\n",
    "        test_group += group\n",
    "\n",
    "    count += len(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43bca8",
   "metadata": {},
   "source": [
    "## Add sentences with only 'O' tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13f40576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add unused sentences with all 'O' tags\n",
    "used = set(train_group + dev_group + test_group)\n",
    "o_tagged = []\n",
    "\n",
    "for idx, sent in enumerate(total_data):\n",
    "    if idx not in used and all(tag == \"O\" for tag in sent[\"ner_tags\"]):\n",
    "        o_tagged.append(idx)\n",
    "\n",
    "random.shuffle(o_tagged)\n",
    "\n",
    "cut1, cut2 = int(len(o_tagged) * 0.7), int(len(o_tagged) * 0.85)\n",
    "\n",
    "train_group += o_tagged[:cut1]\n",
    "dev_group += o_tagged[cut1:cut2]\n",
    "test_group += o_tagged[cut2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2dcace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final splits\n",
    "train_data = [total_data[i] for i in sorted(train_group)]\n",
    "dev_data = [total_data[i] for i in sorted(dev_group)]\n",
    "test_data = [total_data[i] for i in sorted(test_group)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49301886",
   "metadata": {},
   "source": [
    "## Check sizes and overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293968f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 3896\n",
      "dev size: 790\n",
      "test size: 826\n",
      "total dataset size: 5512\n"
     ]
    }
   ],
   "source": [
    "# sizes of new datasets\n",
    "print(\"train size:\", len(train_data))\n",
    "print(\"dev size:\", len(dev_data))\n",
    "print(\"test size:\", len(test_data))\n",
    "print(\"total dataset size:\", len(train_data) + len(dev_data) + len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9feb5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens with non-\"O\" labels from each split\n",
    "train_tokens = extract_labeled_tokens(train_data)\n",
    "dev_tokens = extract_labeled_tokens(dev_data)\n",
    "test_tokens = extract_labeled_tokens(test_data)\n",
    "\n",
    "# overlap between datasets\n",
    "train_dev_overlap = train_tokens & dev_tokens\n",
    "dev_test_overlap = dev_tokens & test_tokens\n",
    "train_test_overlap = train_tokens & test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d792edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap between train and dev: 0\n",
      "overlap between dev and test: 0\n",
      "overlap between train and test: 0\n"
     ]
    }
   ],
   "source": [
    "# check for overlap between datasets\n",
    "print('overlap between train and dev:', len(train_dev_overlap))\n",
    "print('overlap between dev and test:', len(dev_test_overlap))\n",
    "print('overlap between train and test:', len(train_test_overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa731c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap between train and ME_dev: 0\n",
      "overlap between train and ME_test: 0\n",
      "overlap between ME_dev and ME_test: 12\n"
     ]
    }
   ],
   "source": [
    "## checking for ME data ##\n",
    "\n",
    "# path to the data files\n",
    "path_me_dev = \"../data/me_data/middle_eastern_dev.tsv\"\n",
    "path_me_test = \"../data/me_data/middle_eastern_test.tsv\"\n",
    "\n",
    "# read in the data\n",
    "me_dev_data = read_tsv_file(path_me_dev, label2id)\n",
    "me_test_data = read_tsv_file(path_me_test, label2id)\n",
    "\n",
    "# extract labels\n",
    "me_dev_tokens = extract_labeled_tokens(me_dev_data)\n",
    "me_test_tokens = extract_labeled_tokens(me_test_data)\n",
    "\n",
    "# overlap between datasets\n",
    "me_train_dev_overlap = train_tokens & me_dev_tokens\n",
    "me_train_test_overlap = train_tokens & me_test_tokens\n",
    "me_dev_test_overlap = me_dev_tokens & me_test_tokens\n",
    "\n",
    "print('overlap between train and ME_dev:', len(me_train_dev_overlap))\n",
    "print('overlap between train and ME_test:', len(me_train_test_overlap))\n",
    "print('overlap between ME_dev and ME_test:', len(me_dev_test_overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbcf1f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'Amarah\",\n",
       " '172',\n",
       " '3ayez',\n",
       " '9000',\n",
       " 'ABC',\n",
       " 'ADNOC',\n",
       " 'AKKASA',\n",
       " 'ALAHLI',\n",
       " 'Abbas',\n",
       " 'Abdel',\n",
       " 'Abdul karim',\n",
       " 'Abdulahi',\n",
       " 'Abdulrahman',\n",
       " 'Abir',\n",
       " 'Ad-Dustour',\n",
       " 'Adab',\n",
       " 'Adeel',\n",
       " 'Adhyby',\n",
       " 'Adil',\n",
       " 'Aezra',\n",
       " 'Africa',\n",
       " 'Aftab',\n",
       " 'Ahar',\n",
       " 'Aiguebelle',\n",
       " 'Aiman',\n",
       " 'Ain',\n",
       " 'Aishah',\n",
       " 'Ajlun',\n",
       " 'Akbar',\n",
       " 'Akdital',\n",
       " 'Al',\n",
       " 'Al khaili',\n",
       " 'Al khaily',\n",
       " 'Al zawaydah',\n",
       " 'Al zuben',\n",
       " 'Al-Ahd',\n",
       " 'Al-Akhdar',\n",
       " 'Al-Arab',\n",
       " 'Al-Bilad',\n",
       " 'Al-Ittihad',\n",
       " 'Al-Kalima',\n",
       " 'Al-Muhaidib',\n",
       " 'Al-Tijari',\n",
       " 'Al-Waie',\n",
       " 'Al-Wehda',\n",
       " 'Al-Yawm',\n",
       " 'Ala',\n",
       " 'Alahmadi',\n",
       " 'Alaidaybi',\n",
       " 'Alaidaysi',\n",
       " 'Alaijayzi',\n",
       " 'Alaishaymi',\n",
       " 'Alaitaybiah',\n",
       " 'Alaizayki',\n",
       " 'Albandari',\n",
       " 'Albanuny',\n",
       " 'Albouhloul',\n",
       " 'Aleama',\n",
       " 'Alia',\n",
       " 'Alisa',\n",
       " 'Allawi',\n",
       " 'Alnawaflh',\n",
       " 'Alsayed',\n",
       " 'Altihayshi',\n",
       " 'Altiwayqi',\n",
       " 'Altiwayshi',\n",
       " 'Alto',\n",
       " 'Amal',\n",
       " 'Amar',\n",
       " 'Amin',\n",
       " 'Ana',\n",
       " 'Anbaa',\n",
       " 'Anika',\n",
       " 'Anjum',\n",
       " 'Anzal-e',\n",
       " 'Arabien',\n",
       " 'Arak',\n",
       " 'Arasco',\n",
       " 'Argaam',\n",
       " 'Arif',\n",
       " 'Arryadia',\n",
       " 'As',\n",
       " 'Ashmun',\n",
       " 'Ashour',\n",
       " 'Asiacell',\n",
       " 'Asmaa',\n",
       " 'Asmidal',\n",
       " 'Asra',\n",
       " 'Assabeel',\n",
       " 'Assadissa',\n",
       " 'Athaqafia',\n",
       " 'Atif',\n",
       " 'Attajdid',\n",
       " 'Automobiles',\n",
       " 'Ayvalık',\n",
       " 'Azad',\n",
       " 'Azhar',\n",
       " 'Azzam',\n",
       " 'Ba Baryja',\n",
       " 'Ba Buayr',\n",
       " 'Bader',\n",
       " 'Bahri',\n",
       " 'Bakdash',\n",
       " 'Bakkens',\n",
       " 'Baneh',\n",
       " 'Bank',\n",
       " 'Bashir',\n",
       " 'Bassem',\n",
       " 'Batelco',\n",
       " 'Batoul',\n",
       " 'Bawshar',\n",
       " 'Başakşehir',\n",
       " 'Beirut',\n",
       " 'Belek',\n",
       " 'Bibi',\n",
       " 'Bimo',\n",
       " 'Binaabdlah',\n",
       " 'Bitar',\n",
       " 'Bojnurd',\n",
       " 'Borouge',\n",
       " 'Borujerd',\n",
       " 'Brødrene',\n",
       " 'Buraydah',\n",
       " 'Cairo',\n",
       " 'Cairo-step',\n",
       " 'Capital',\n",
       " 'Carola-jakker',\n",
       " 'Cemal',\n",
       " 'Cessna',\n",
       " 'Chefaa',\n",
       " 'Chlorella',\n",
       " 'Company',\n",
       " 'Covenant',\n",
       " 'Crescent',\n",
       " 'Croma',\n",
       " 'Crusoe',\n",
       " 'DEWA',\n",
       " 'DM',\n",
       " 'DMC',\n",
       " 'DTM-sæson',\n",
       " 'DUPLO',\n",
       " 'Dah',\n",
       " 'Dalila',\n",
       " 'Damaskus',\n",
       " 'Damghan',\n",
       " 'Damietta',\n",
       " 'Damir',\n",
       " 'Dammam',\n",
       " 'Dannebrogsflag',\n",
       " 'Danube',\n",
       " 'Daraa',\n",
       " 'Darab',\n",
       " 'Daraty',\n",
       " 'Daria',\n",
       " 'DenizBank',\n",
       " 'Denizli',\n",
       " 'Desai',\n",
       " 'Didan',\n",
       " 'Disuq',\n",
       " 'Diyarbakir',\n",
       " 'Djezzy',\n",
       " 'Dnata',\n",
       " 'Douma',\n",
       " 'Doğubayazıt',\n",
       " 'Dubai',\n",
       " 'Dukhan',\n",
       " 'Echorouk',\n",
       " 'Eel',\n",
       " 'EgyptAir',\n",
       " 'Egyptalum',\n",
       " 'El shatti',\n",
       " 'Electric',\n",
       " 'Eleva2ren',\n",
       " 'Emaar',\n",
       " 'Emirates',\n",
       " 'Erbil',\n",
       " 'Ereğli',\n",
       " 'Erzurum',\n",
       " 'Esenyurt',\n",
       " 'Etisalat',\n",
       " 'Europa-Cup-turneringen',\n",
       " 'Fadia',\n",
       " 'Fadil',\n",
       " 'Fajr',\n",
       " 'Fanta',\n",
       " 'Farhad',\n",
       " 'Faris',\n",
       " 'Fatema',\n",
       " 'Faten',\n",
       " 'Fathi',\n",
       " 'Fatuma',\n",
       " 'Faw',\n",
       " 'Flynas',\n",
       " 'Fouad',\n",
       " 'Furkan',\n",
       " 'Fylla',\n",
       " 'G42',\n",
       " 'GMO',\n",
       " \"GMO'er\",\n",
       " 'Gani',\n",
       " 'Gebze',\n",
       " 'Gemlik',\n",
       " 'George',\n",
       " 'Ghassan',\n",
       " 'Ghazi',\n",
       " 'Glass',\n",
       " 'Gorgan',\n",
       " 'Group',\n",
       " 'Gupta',\n",
       " 'Hadi',\n",
       " 'Hajar',\n",
       " 'Hakami',\n",
       " 'Hala',\n",
       " 'HalalaH',\n",
       " 'Halim',\n",
       " 'Hamdi',\n",
       " 'Hamed',\n",
       " 'Hana',\n",
       " 'Harun',\n",
       " 'Hash',\n",
       " 'Hashem',\n",
       " 'Hasiba',\n",
       " 'Hatun',\n",
       " 'Hawra',\n",
       " 'Hawraa',\n",
       " 'Hellig',\n",
       " 'Hicham',\n",
       " 'Hilal',\n",
       " 'Hisham',\n",
       " 'Homs',\n",
       " 'Hufuf',\n",
       " 'Hurghada',\n",
       " 'Hvile',\n",
       " 'Idlib',\n",
       " 'Ikram',\n",
       " 'Ilias',\n",
       " 'Illustreret',\n",
       " 'Imane',\n",
       " 'International',\n",
       " 'Investcorp',\n",
       " 'Investments',\n",
       " 'Irbid',\n",
       " 'Isafold',\n",
       " 'Isam',\n",
       " 'Isra',\n",
       " 'Izmir',\n",
       " 'Jamahir',\n",
       " 'Jamal',\n",
       " 'Jamalon',\n",
       " 'Jamil',\n",
       " 'Jamila',\n",
       " 'Jamilla',\n",
       " 'Jammaliyah',\n",
       " 'Jarrar',\n",
       " 'Javed',\n",
       " 'Jawad',\n",
       " 'Jawan',\n",
       " 'Jidda',\n",
       " 'Jihad',\n",
       " 'Jihan',\n",
       " 'Jinan',\n",
       " 'Jirja',\n",
       " 'Jonubi',\n",
       " 'Junes',\n",
       " 'Justy',\n",
       " 'K24',\n",
       " 'KIPCO',\n",
       " 'Kahramaa',\n",
       " 'Kamil',\n",
       " 'Karabağlar',\n",
       " 'Karmil',\n",
       " 'Kermanshah',\n",
       " 'Kesmou Elkerim',\n",
       " 'Khadra',\n",
       " 'Khalida',\n",
       " 'Khalifa',\n",
       " 'Khalis',\n",
       " 'Khanjarah',\n",
       " 'Khorramabad',\n",
       " 'Kodimagnyl',\n",
       " 'Konger',\n",
       " 'Konya',\n",
       " 'Kuşadası',\n",
       " 'Kystland',\n",
       " 'Körfez',\n",
       " 'Køkkengudens',\n",
       " 'Kızıltepe',\n",
       " 'Langarud',\n",
       " 'Latakia',\n",
       " 'Legacy',\n",
       " 'Leone',\n",
       " 'Leyla',\n",
       " 'Libyana',\n",
       " 'Liwaa',\n",
       " 'Løvehjerte',\n",
       " 'Maastricht-aftalerne',\n",
       " 'Mada',\n",
       " 'Mahamed',\n",
       " 'Mahdi',\n",
       " 'Mahnaz',\n",
       " 'Majida',\n",
       " 'Makki',\n",
       " 'Malek',\n",
       " 'Mallawi',\n",
       " 'Man',\n",
       " 'Manar',\n",
       " 'Manavgat',\n",
       " 'Marafiq',\n",
       " 'Mardin',\n",
       " 'Maria',\n",
       " 'Mariam',\n",
       " 'Marjane',\n",
       " 'Maroc',\n",
       " 'MarsaMaroc',\n",
       " 'Marwa',\n",
       " 'Mashreq',\n",
       " 'Masraf',\n",
       " 'Masri',\n",
       " 'Matariyah',\n",
       " 'Mazazikh',\n",
       " 'Medicine',\n",
       " 'Mehmed',\n",
       " 'Mekka',\n",
       " 'Mia',\n",
       " 'Miandoab',\n",
       " 'Mikail',\n",
       " 'Mobily',\n",
       " 'Mohamad',\n",
       " 'Motorrens',\n",
       " 'Moumadi',\n",
       " 'Mounir',\n",
       " 'Mubadala',\n",
       " 'Muhamed',\n",
       " 'Muhammad',\n",
       " 'Mukalla',\n",
       " 'Mumadi',\n",
       " 'Murtaza',\n",
       " 'Musa',\n",
       " 'Muscat',\n",
       " 'Mushtaq',\n",
       " 'Mustafakemalpaşa',\n",
       " 'NBB',\n",
       " 'NBN',\n",
       " 'Nabila',\n",
       " 'Nablus',\n",
       " 'Nabooda',\n",
       " 'Nadia',\n",
       " 'Nahid',\n",
       " 'Nail',\n",
       " 'Najla',\n",
       " 'Najwa',\n",
       " 'Naked',\n",
       " 'Nareva',\n",
       " 'Nasser',\n",
       " 'National',\n",
       " 'Nature',\n",
       " 'Naveed',\n",
       " 'Nazif',\n",
       " 'Nazilli',\n",
       " 'Nejla',\n",
       " 'New',\n",
       " 'Nilesat',\n",
       " 'Noon',\n",
       " 'Nur',\n",
       " 'Nura',\n",
       " 'ONCF',\n",
       " 'Of',\n",
       " 'Okaz',\n",
       " 'Oktophonie',\n",
       " 'Ola',\n",
       " 'Omdurman',\n",
       " 'Ooredoo',\n",
       " 'Oscar',\n",
       " 'Pamol',\n",
       " 'Pandey',\n",
       " 'Panodil',\n",
       " 'Parken',\n",
       " 'Party',\n",
       " 'Patchi',\n",
       " 'Petrochemical',\n",
       " 'Properties',\n",
       " 'PubliTools',\n",
       " 'QNB',\n",
       " 'Qarchak',\n",
       " 'Qasimu Alfadil',\n",
       " 'Qasimulfadil',\n",
       " 'Qurayyat',\n",
       " 'RASCO',\n",
       " 'Raad',\n",
       " 'Radia',\n",
       " 'Radwan',\n",
       " 'Rafsanjan',\n",
       " 'Rahima',\n",
       " 'Rahlah',\n",
       " 'Rahman',\n",
       " 'Rai',\n",
       " 'Ramadi',\n",
       " 'Ramin',\n",
       " 'Ramzi',\n",
       " 'Raouf',\n",
       " 'Ras',\n",
       " 'Rashed',\n",
       " 'Rayaam',\n",
       " 'Rayan',\n",
       " 'Red',\n",
       " 'Reza',\n",
       " 'Rezayat',\n",
       " 'Ridha',\n",
       " 'Riyad',\n",
       " 'Rize',\n",
       " 'Robinson',\n",
       " 'SGTM',\n",
       " 'SNRT',\n",
       " 'SNVI',\n",
       " 'SOMED',\n",
       " 'SPORTS',\n",
       " 'Saab',\n",
       " 'Saad',\n",
       " 'Sadia',\n",
       " 'Saeeda',\n",
       " 'Safaa',\n",
       " 'Safia',\n",
       " 'Sahar',\n",
       " 'Sahara',\n",
       " 'Sahban',\n",
       " 'Sajid',\n",
       " 'Sajida',\n",
       " 'Saleh',\n",
       " 'Salem',\n",
       " 'Salima',\n",
       " 'Samad',\n",
       " 'Saman',\n",
       " 'Samawah',\n",
       " 'Samed',\n",
       " 'Samia',\n",
       " 'Samina',\n",
       " 'Sanandaj',\n",
       " 'Sancaktepe',\n",
       " 'Sarwa',\n",
       " 'Saudi',\n",
       " 'Saudia',\n",
       " 'Sawaida',\n",
       " 'Science',\n",
       " 'Seat',\n",
       " 'Sela',\n",
       " 'Seyed',\n",
       " 'Shadi',\n",
       " 'Shahid',\n",
       " 'Shahnaz',\n",
       " 'Shahrud',\n",
       " 'Shahrzad',\n",
       " 'Shahzad',\n",
       " 'Shaima',\n",
       " 'Shamli',\n",
       " 'Shamsina',\n",
       " 'Sharjah',\n",
       " 'Shorouk',\n",
       " 'Sidra',\n",
       " 'Sinan',\n",
       " 'Siuaraa',\n",
       " 'Siwaar',\n",
       " 'Sodaa',\n",
       " 'Solfeh',\n",
       " 'Solus',\n",
       " 'Somaca',\n",
       " 'Sonalgaz',\n",
       " 'Sonasid',\n",
       " 'Sorgun',\n",
       " 'Souqalmal',\n",
       " 'Spinneys',\n",
       " 'Stokken',\n",
       " 'Sulayman',\n",
       " 'Sultanah',\n",
       " 'Sundus',\n",
       " 'Susan',\n",
       " 'Söke',\n",
       " 'TOOLO',\n",
       " \"Ta'if\",\n",
       " 'Tahir',\n",
       " 'Tai',\n",
       " 'Taj',\n",
       " 'Talat',\n",
       " 'Tamatem',\n",
       " 'Tarjama',\n",
       " 'Telecom',\n",
       " 'Thabet',\n",
       " 'Thunder',\n",
       " 'Tishreen',\n",
       " 'Toledo',\n",
       " 'Toolbox',\n",
       " 'Torath',\n",
       " 'Tre',\n",
       " 'Treo',\n",
       " 'Turgutlu',\n",
       " 'Tyrkiet',\n",
       " 'Ul’Jadid',\n",
       " 'Umair',\n",
       " 'Usman',\n",
       " 'Van',\n",
       " 'Videnskabs',\n",
       " 'Viranşehir',\n",
       " 'Wael',\n",
       " 'Wafa',\n",
       " 'Wahida',\n",
       " 'Wanasah',\n",
       " 'Waqar',\n",
       " 'Waqas',\n",
       " 'Waseem',\n",
       " 'Xenel',\n",
       " 'Yakub',\n",
       " 'Yasmin',\n",
       " 'Yasmine',\n",
       " 'Yassin',\n",
       " 'Yassine',\n",
       " 'Yazd',\n",
       " 'Yonas',\n",
       " 'Younes',\n",
       " 'Yousif',\n",
       " 'Yusef',\n",
       " 'Zabol',\n",
       " 'Zagazig',\n",
       " 'Zahraa',\n",
       " 'Zaid',\n",
       " 'Zaina',\n",
       " 'Zaki',\n",
       " 'Ziad',\n",
       " 'Ziba',\n",
       " 'ad',\n",
       " 'al-Fajr',\n",
       " 'al-Jadid',\n",
       " 'al-Jamahiriyah',\n",
       " 'al-Watan',\n",
       " 'al-Zahf',\n",
       " 'anden',\n",
       " 'beIN',\n",
       " 'bistandshjælp',\n",
       " 'dannebrogsflag',\n",
       " 'dollar',\n",
       " 'eSpace',\n",
       " 'fåresyge',\n",
       " 'gonorré',\n",
       " 'hustru',\n",
       " 'kunstfond',\n",
       " 'op',\n",
       " 'sejle',\n",
       " 'statens',\n",
       " 'verdenskrig',\n",
       " 'Çubuk',\n",
       " 'Ørestadsbanen',\n",
       " 'åen',\n",
       " 'Şişli'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe09a24",
   "metadata": {},
   "source": [
    "## Look at distribution of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf67cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'I-PER': 572, 'B-PER': 455, 'B-ORG': 438, 'B-LOC': 347, 'I-ORG': 323, 'I-MISC': 206, 'B-MISC': 166, 'I-LOC': 61})\n",
      "Counter({'B-PER': 98, 'B-ORG': 80, 'B-LOC': 78, 'B-MISC': 54, 'I-PER': 37, 'I-MISC': 22, 'I-ORG': 14, 'I-LOC': 5})\n",
      "Counter({'B-PER': 131, 'B-ORG': 110, 'B-LOC': 75, 'I-PER': 60, 'B-MISC': 56, 'I-ORG': 40, 'I-MISC': 19, 'I-LOC': 10})\n"
     ]
    }
   ],
   "source": [
    "train_tokens = extract_labeled_tokens(train_data, include_label_pair=True)\n",
    "dev_tokens = extract_labeled_tokens(dev_data, include_label_pair=True)\n",
    "test_tokens = extract_labeled_tokens(test_data, include_label_pair=True)\n",
    "\n",
    "train_distr = Counter(tag for _, tag in train_tokens)\n",
    "test_distr = Counter(tag for _, tag in test_tokens)\n",
    "dev_distr = Counter(tag for _, tag in dev_tokens)\n",
    "\n",
    "print(train_distr)\n",
    "print(dev_distr)\n",
    "print(test_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "091a92ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Percentage Distribution:\n",
      "{'I-PER': 22.27, 'B-LOC': 13.51, 'I-ORG': 12.58, 'B-PER': 17.72, 'B-MISC': 6.46, 'B-ORG': 17.06, 'I-LOC': 2.38, 'I-MISC': 8.02}\n",
      "\n",
      "Dev Percentage Distribution:\n",
      "{'B-ORG': 20.62, 'B-LOC': 20.1, 'B-PER': 25.26, 'I-PER': 9.54, 'I-ORG': 3.61, 'B-MISC': 13.92, 'I-MISC': 5.67, 'I-LOC': 1.29}\n",
      "\n",
      "Test Percentage Distribution:\n",
      "{'I-ORG': 7.98, 'B-MISC': 11.18, 'B-PER': 26.15, 'I-PER': 11.98, 'B-ORG': 21.96, 'B-LOC': 14.97, 'I-LOC': 2.0, 'I-MISC': 3.79}\n"
     ]
    }
   ],
   "source": [
    "def get_percentage_distribution(counter):\n",
    "    total = sum(counter.values())\n",
    "    return {tag: round((count / total) * 100, 2) for tag, count in counter.items()}\n",
    "\n",
    "# calculate percentage distributions\n",
    "train_percent = get_percentage_distribution(train_distr)\n",
    "dev_percent = get_percentage_distribution(dev_distr)\n",
    "test_percent = get_percentage_distribution(test_distr)\n",
    "\n",
    "# print results\n",
    "print(\"Train Percentage Distribution:\")\n",
    "print(train_percent)\n",
    "print(\"\\nDev Percentage Distribution:\")\n",
    "print(dev_percent)\n",
    "print(\"\\nTest Percentage Distribution:\")\n",
    "print(test_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a4e97",
   "metadata": {},
   "source": [
    "## Write to tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "219b29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tsv_file(train_data, '../data/no_overlap_da_news/da_news_train.tsv')\n",
    "write_tsv_file(dev_data, '../data/no_overlap_da_news/da_news_dev.tsv')\n",
    "write_tsv_file(test_data, '../data/no_overlap_da_news/da_news_test.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
