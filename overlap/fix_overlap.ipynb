{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b06cb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.load_data import mapping, read_tsv_file, extract_labeled_tokens\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2fd6d",
   "metadata": {},
   "source": [
    "## Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "83b7cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_news_train = \"../data/da_news/da_news_train.tsv\"\n",
    "path_news_dev = \"../data/da_news/da_news_dev.tsv\"\n",
    "path_news_test = \"../data/da_news/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_news_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data_news = read_tsv_file(path_news_train, label2id)\n",
    "dev_data_news = read_tsv_file(path_news_dev, label2id)\n",
    "test_data_news = read_tsv_file(path_news_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd36e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 4383\n",
      "dev size: 564\n",
      "test size: 565\n",
      "total dataset size: 5512\n"
     ]
    }
   ],
   "source": [
    "# dataset sizes\n",
    "print(\"train size:\", len(train_data_news))\n",
    "print(\"dev size:\", len(dev_data_news))\n",
    "print(\"test size:\", len(test_data_news))\n",
    "print(\"total dataset size:\", len(train_data_news) + len(dev_data_news) + len(test_data_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35ffd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate datasets\n",
    "total_data = train_data_news + dev_data_news + test_data_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f218abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraxt unique entities\n",
    "total_entities = extract_labeled_tokens(total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97ac14",
   "metadata": {},
   "source": [
    "## Build mapping from entity to sentence and sentence to entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4144de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict with entities as keys and lists of sentence IDs as values\n",
    "entity_to_sents = defaultdict(set)\n",
    "sent_to_entities = defaultdict(set) # also creating mapping from sentence ID to entity\n",
    "\n",
    "for sent_id, sent in enumerate(total_data):\n",
    "\n",
    "    for tok_id, ent in enumerate(sent[\"tokens\"]):\n",
    "\n",
    "        if ent in total_entities and sent['ner_tags'][tok_id] != 'O':\n",
    "\n",
    "            entity_to_sents[ent].add(sent_id)\n",
    "\n",
    "            sent_to_entities[sent_id].add(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89defe",
   "metadata": {},
   "source": [
    "## Group sentences by overlapping entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "40ecab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group sentences by shared entities\n",
    "\n",
    "visited = set()\n",
    "sentence_groups = []\n",
    "\n",
    "for sent_id in sent_to_entities:\n",
    "\n",
    "    if sent_id in visited:\n",
    "        continue\n",
    "\n",
    "    group, queue = set(), [sent_id]\n",
    "\n",
    "    while queue:\n",
    "\n",
    "        current = queue.pop()\n",
    "\n",
    "        if current in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(current)\n",
    "        group.add(current)\n",
    "\n",
    "        for entity in sent_to_entities[current]:\n",
    "\n",
    "            queue.extend(entity_to_sents[entity])\n",
    "\n",
    "    sentence_groups.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7abb0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split groups by total sentence count\n",
    "\n",
    "random.shuffle(sentence_groups)\n",
    "\n",
    "train_group, dev_group, test_group, count = [], [], [], 0\n",
    "total = sum(len(g) for g in sentence_groups)\n",
    "train_cutoff, dev_cutoff = int(total * 0.7), int(total * 0.85)\n",
    "\n",
    "for group in sentence_groups:\n",
    "\n",
    "    if count < train_cutoff:\n",
    "        train_group += group\n",
    "        \n",
    "    elif count < dev_cutoff:\n",
    "        dev_group += group\n",
    "\n",
    "    else:\n",
    "        test_group += group\n",
    "\n",
    "    count += len(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43bca8",
   "metadata": {},
   "source": [
    "## Add sentences with only 'O' tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "13f40576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add unused sentences with all 'O' tags\n",
    "used = set(train_group + dev_group + test_group)\n",
    "o_tagged = []\n",
    "\n",
    "for idx, sent in enumerate(total_data):\n",
    "    if idx not in used and all(tag == \"O\" for tag in sent[\"ner_tags\"]):\n",
    "        o_tagged.append(idx)\n",
    "\n",
    "random.shuffle(o_tagged)\n",
    "\n",
    "cut1, cut2 = int(len(o_tagged) * 0.7), int(len(o_tagged) * 0.85)\n",
    "\n",
    "train_group += o_tagged[:cut1]\n",
    "dev_group += o_tagged[cut1:cut2]\n",
    "test_group += o_tagged[cut2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e2dcace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final splits\n",
    "train_data = [total_data[i] for i in sorted(train_group)]\n",
    "dev_data = [total_data[i] for i in sorted(dev_group)]\n",
    "test_data = [total_data[i] for i in sorted(test_group)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49301886",
   "metadata": {},
   "source": [
    "## Check sizes and overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "293968f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 4270\n",
      "dev size: 489\n",
      "test size: 753\n",
      "total dataset size: 5512\n"
     ]
    }
   ],
   "source": [
    "# sizes of new datasets\n",
    "print(\"train size:\", len(train_data))\n",
    "print(\"dev size:\", len(dev_data))\n",
    "print(\"test size:\", len(test_data))\n",
    "print(\"total dataset size:\", len(train_data) + len(dev_data) + len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9feb5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens with non-\"O\" labels from each split\n",
    "train_tokens = extract_labeled_tokens(train_data)\n",
    "dev_tokens = extract_labeled_tokens(dev_data)\n",
    "test_tokens = extract_labeled_tokens(test_data)\n",
    "\n",
    "# overlap between datasets\n",
    "train_dev_overlap = train_tokens & dev_tokens\n",
    "dev_test_overlap = dev_tokens & test_tokens\n",
    "train_test_overlap = train_tokens & test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d792edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap between train and dev: 0\n",
      "overlap between dev and test: 0\n",
      "overlap between train and test: 0\n"
     ]
    }
   ],
   "source": [
    "# check for overlap between datasets\n",
    "print('overlap between train and dev:', len(train_dev_overlap))\n",
    "print('overlap between dev and test:', len(dev_test_overlap))\n",
    "print('overlap between train and test:', len(train_test_overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a4e97",
   "metadata": {},
   "source": [
    "## Write to tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "329d8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tsv_file(data, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in data:\n",
    "            tokens = sentence['tokens']\n",
    "            ner_tags = sentence['ner_tags']\n",
    "            for token, tag in zip(tokens, ner_tags):\n",
    "                f.write(f\"{token}\\t{tag}\\n\")\n",
    "            f.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "219b29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tsv_file(train_data, '../data/no_overlap_da_news/da_news_train.tsv')\n",
    "write_tsv_file(dev_data, '../data/no_overlap_da_news/da_news_dev.tsv')\n",
    "write_tsv_file(test_data, '../data/no_overlap_da_news/da_news_test.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
