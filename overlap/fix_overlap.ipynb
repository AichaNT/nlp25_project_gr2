{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.load_data import mapping, extract_labeled_tokens, read_tsv_file, write_tsv_file\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "random.seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2fd6d",
   "metadata": {},
   "source": [
    "## Load original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "83b7cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data files\n",
    "path_news_train = \"../data/da_news/da_news_train.tsv\"\n",
    "path_news_dev = \"../data/da_news/da_news_dev.tsv\"\n",
    "path_news_test = \"../data/da_news/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_news_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data_news = read_tsv_file(path_news_train, label2id)\n",
    "dev_data_news = read_tsv_file(path_news_dev, label2id)\n",
    "test_data_news = read_tsv_file(path_news_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "dd36e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 4383\n",
      "dev size: 564\n",
      "test size: 565\n",
      "total dataset size: 5512\n"
     ]
    }
   ],
   "source": [
    "# dataset sizes\n",
    "print(\"train size:\", len(train_data_news))\n",
    "print(\"dev size:\", len(dev_data_news))\n",
    "print(\"test size:\", len(test_data_news))\n",
    "print(\"total dataset size:\", len(train_data_news) + len(dev_data_news) + len(test_data_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "35ffd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate datasets\n",
    "total_data = train_data_news + dev_data_news + test_data_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f218abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraxt unique entities\n",
    "total_entities = extract_labeled_tokens(total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97ac14",
   "metadata": {},
   "source": [
    "## Build mapping from entity to sentence and sentence to entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4144de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict with entities as keys and lists of sentence IDs as values\n",
    "entity_to_sents = defaultdict(set)\n",
    "sent_to_entities = defaultdict(set) # also creating mapping from sentence ID to entity\n",
    "\n",
    "for sent_id, sent in enumerate(total_data):\n",
    "\n",
    "    for tok_id, ent in enumerate(sent[\"tokens\"]):\n",
    "\n",
    "        if ent in total_entities and sent['ner_tags'][tok_id] != 'O':\n",
    "\n",
    "            entity_to_sents[ent].add(sent_id)\n",
    "\n",
    "            sent_to_entities[sent_id].add(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89defe",
   "metadata": {},
   "source": [
    "## Group sentences by overlapping entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "40ecab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group sentences by shared entities\n",
    "\n",
    "visited = set()\n",
    "sentence_groups = []\n",
    "\n",
    "for sent_id in sent_to_entities:\n",
    "\n",
    "    if sent_id in visited:\n",
    "        continue\n",
    "\n",
    "    group, queue = set(), [sent_id]\n",
    "\n",
    "    while queue:\n",
    "\n",
    "        current = queue.pop()\n",
    "\n",
    "        if current in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(current)\n",
    "        group.add(current)\n",
    "\n",
    "        for entity in sent_to_entities[current]:\n",
    "\n",
    "            queue.extend(entity_to_sents[entity])\n",
    "\n",
    "    sentence_groups.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "7abb0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split groups by total sentence count\n",
    "\n",
    "random.shuffle(sentence_groups)\n",
    "\n",
    "train_group, dev_group, test_group, count = [], [], [], 0\n",
    "total = sum(len(g) for g in sentence_groups)\n",
    "train_cutoff, dev_cutoff = int(total * 0.7), int(total * 0.85)\n",
    "\n",
    "for group in sentence_groups:\n",
    "\n",
    "    if count < train_cutoff:\n",
    "        train_group += group\n",
    "        \n",
    "    elif count < dev_cutoff:\n",
    "        dev_group += group\n",
    "\n",
    "    else:\n",
    "        test_group += group\n",
    "\n",
    "    count += len(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43bca8",
   "metadata": {},
   "source": [
    "## Add sentences with only 'O' tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "13f40576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add unused sentences with all 'O' tags\n",
    "used = set(train_group + dev_group + test_group)\n",
    "o_tagged = []\n",
    "\n",
    "for idx, sent in enumerate(total_data):\n",
    "    if idx not in used and all(tag == \"O\" for tag in sent[\"ner_tags\"]):\n",
    "        o_tagged.append(idx)\n",
    "\n",
    "random.shuffle(o_tagged)\n",
    "\n",
    "cut1, cut2 = int(len(o_tagged) * 0.7), int(len(o_tagged) * 0.85)\n",
    "\n",
    "train_group += o_tagged[:cut1]\n",
    "dev_group += o_tagged[cut1:cut2]\n",
    "test_group += o_tagged[cut2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e2dcace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final splits\n",
    "train_data = [total_data[i] for i in sorted(train_group)]\n",
    "dev_data = [total_data[i] for i in sorted(dev_group)]\n",
    "test_data = [total_data[i] for i in sorted(test_group)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49301886",
   "metadata": {},
   "source": [
    "## Check sizes and overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "293968f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 3896\n",
      "dev size: 790\n",
      "test size: 826\n",
      "total dataset size: 5512\n"
     ]
    }
   ],
   "source": [
    "# sizes of new datasets\n",
    "print(\"train size:\", len(train_data))\n",
    "print(\"dev size:\", len(dev_data))\n",
    "print(\"test size:\", len(test_data))\n",
    "print(\"total dataset size:\", len(train_data) + len(dev_data) + len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "9feb5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens with non-\"O\" labels from each split\n",
    "train_tokens = extract_labeled_tokens(train_data)\n",
    "dev_tokens = extract_labeled_tokens(dev_data)\n",
    "test_tokens = extract_labeled_tokens(test_data)\n",
    "\n",
    "# overlap between datasets\n",
    "train_dev_overlap = train_tokens & dev_tokens\n",
    "dev_test_overlap = dev_tokens & test_tokens\n",
    "train_test_overlap = train_tokens & test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d792edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap between train and dev: 0\n",
      "overlap between dev and test: 0\n",
      "overlap between train and test: 0\n"
     ]
    }
   ],
   "source": [
    "# check for overlap between datasets\n",
    "print('overlap between train and dev:', len(train_dev_overlap))\n",
    "print('overlap between dev and test:', len(dev_test_overlap))\n",
    "print('overlap between train and test:', len(train_test_overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe09a24",
   "metadata": {},
   "source": [
    "## Look at distribution of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'I-PER': 572, 'B-PER': 455, 'B-ORG': 438, 'B-LOC': 347, 'I-ORG': 323, 'I-MISC': 206, 'B-MISC': 166, 'I-LOC': 61})\n",
      "Counter({'B-PER': 98, 'B-ORG': 80, 'B-LOC': 78, 'B-MISC': 54, 'I-PER': 37, 'I-MISC': 22, 'I-ORG': 14, 'I-LOC': 5})\n",
      "Counter({'B-PER': 131, 'B-ORG': 110, 'B-LOC': 75, 'I-PER': 60, 'B-MISC': 56, 'I-ORG': 40, 'I-MISC': 19, 'I-LOC': 10})\n"
     ]
    }
   ],
   "source": [
    "train_tokens = extract_labeled_tokens(train_data, include_label_pair=True)\n",
    "dev_tokens = extract_labeled_tokens(dev_data, include_label_pair=True)\n",
    "test_tokens = extract_labeled_tokens(test_data, include_label_pair=True)\n",
    "\n",
    "train_distr = Counter(tag for _, tag in train_tokens)\n",
    "test_distr = Counter(tag for _, tag in test_tokens)\n",
    "dev_distr = Counter(tag for _, tag in dev_tokens)\n",
    "\n",
    "print(train_distr)\n",
    "print(dev_distr)\n",
    "print(test_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "091a92ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Percentage Distribution:\n",
      "{'B-PER': 17.72, 'I-PER': 22.27, 'I-ORG': 12.58, 'B-MISC': 6.46, 'B-ORG': 17.06, 'B-LOC': 13.51, 'I-MISC': 8.02, 'I-LOC': 2.38}\n",
      "\n",
      "Dev Percentage Distribution:\n",
      "{'B-PER': 25.26, 'B-MISC': 13.92, 'B-ORG': 20.62, 'I-MISC': 5.67, 'B-LOC': 20.1, 'I-PER': 9.54, 'I-LOC': 1.29, 'I-ORG': 3.61}\n",
      "\n",
      "Test Percentage Distribution:\n",
      "{'I-MISC': 3.79, 'B-ORG': 21.96, 'B-PER': 26.15, 'I-PER': 11.98, 'B-LOC': 14.97, 'B-MISC': 11.18, 'I-ORG': 7.98, 'I-LOC': 2.0}\n"
     ]
    }
   ],
   "source": [
    "def get_percentage_distribution(counter):\n",
    "    total = sum(counter.values())\n",
    "    return {tag: round((count / total) * 100, 2) for tag, count in counter.items()}\n",
    "\n",
    "# calculate percentage distributions\n",
    "train_percent = get_percentage_distribution(train_distr)\n",
    "dev_percent = get_percentage_distribution(dev_distr)\n",
    "test_percent = get_percentage_distribution(test_distr)\n",
    "\n",
    "# print results\n",
    "print(\"Train Percentage Distribution:\")\n",
    "print(train_percent)\n",
    "print(\"\\nDev Percentage Distribution:\")\n",
    "print(dev_percent)\n",
    "print(\"\\nTest Percentage Distribution:\")\n",
    "print(test_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a4e97",
   "metadata": {},
   "source": [
    "## Write to tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "219b29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tsv_file(train_data, '../data/no_overlap_da_news/da_news_train.tsv')\n",
    "write_tsv_file(dev_data, '../data/no_overlap_da_news/da_news_dev.tsv')\n",
    "write_tsv_file(test_data, '../data/no_overlap_da_news/da_news_test.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
