{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "81e11d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.load_data import mapping, read_tsv_file\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bd2cd",
   "metadata": {},
   "source": [
    "### Remeber to change extract_labeled_tokens in load_data.py to return list rather than set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ea675aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting tokens to check for overlap in train, dev and test sets\n",
    "def extract_labeled_tokens(dataset, exclude_label = \"O\", include_label_pair=False):\n",
    "    '''\n",
    "    This function extracts tokens from a dataset that have a string label different from `exclude_label`.\n",
    "    Optionally, it can return the (token, label) pairs instead of just tokens.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (List[dict]): The token-tagged dataset.\n",
    "        exclude_label (str): The label to ignore (default is 'O').\n",
    "        include_label_pair (bool): Whether to include the (token, label) pairs in the result (default is False).\n",
    "        \n",
    "    Returns:\n",
    "         Set[str] or Set[Tuple[str, str]]: \n",
    "            - A set of tokens with meaningful (non-O) labels if `include_label_pair` is False.\n",
    "            - A set of (token, label) pairs if `include_label_pair` is True.\n",
    "    '''\n",
    "\n",
    "    # create empty set to store the unique tokens\n",
    "    labeled_tokens = list()\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        # iterate over each token and its corresponding tag ID\n",
    "        for token, label in zip(sentence[\"tokens\"], sentence[\"ner_tags\"]):\n",
    "            if label != exclude_label:                      # check if the tag is not the excluded one\n",
    "                if include_label_pair:\n",
    "                    labeled_tokens.append((token, label))      # add (token, label) pair if the flag is True\n",
    "                else:\n",
    "                    labeled_tokens.append(token)               # add just the token if the flag is False\n",
    "    \n",
    "    return labeled_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "63dd271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data \n",
    "# path to the data files\n",
    "path_train = \"../data/da_news/da_news_train.tsv\"\n",
    "path_dev = \"../data/da_news/da_news_dev.tsv\"\n",
    "path_test = \"../data/da_news/da_news_test.tsv\"\n",
    "\n",
    "# create mapping\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "# read in the DaN+ data\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "936d41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  4383\n",
      "dev size:  564\n",
      "test size:  565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5512"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train size: \", len(train_data))\n",
    "print(\"dev size: \", len(dev_data))\n",
    "print(\"test size: \", len(test_data))\n",
    "len(train_data) + len(dev_data) + len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "824dd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens with non-\"O\" labels from each split\n",
    "train_tokens = extract_labeled_tokens(train_data)\n",
    "dev_tokens = extract_labeled_tokens(dev_data)\n",
    "test_tokens = extract_labeled_tokens(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "25dc1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving as sets\n",
    "train_token_set = set(train_tokens)\n",
    "dev_token_set = set(dev_tokens)\n",
    "test_token_set = set(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "22b7fa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in train : 4820 tokens\n",
      "Tokens in dev : 573 tokens\n",
      "Tokens in test : 695 tokens\n"
     ]
    }
   ],
   "source": [
    "# print out the number of tokens in each split\n",
    "print(f\"Tokens in train : {len(train_tokens)} tokens\")\n",
    "print(f\"Tokens in dev : {len(dev_tokens)} tokens\")\n",
    "print(f\"Tokens in test : {len(test_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a76313c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokes in train : 2635 tokens\n",
      "Unique tokes in dev : 470 tokens\n",
      "Unique tokes in test : 513 tokens\n"
     ]
    }
   ],
   "source": [
    "# print out the number of unique tokens in each split\n",
    "print(f\"Unique tokes in train : {len(train_token_set)} tokens\")\n",
    "print(f\"Unique tokes in dev : {len(dev_token_set)} tokens\")\n",
    "print(f\"Unique tokes in test : {len(test_token_set)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "749d483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to Counters (multisets)\n",
    "train_counter = Counter(train_tokens)\n",
    "dev_counter = Counter(dev_tokens)\n",
    "test_counter = Counter(test_tokens)\n",
    "\n",
    "# compute multiset (duplicate-aware) intersections\n",
    "train_dev_overlap = train_counter & dev_counter\n",
    "train_test_overlap = train_counter & test_counter\n",
    "dev_test_overlap = dev_counter & test_counter\n",
    "all_three_overlap = train_counter & dev_counter & test_counter\n",
    "\n",
    "# convert Counters back to lists if you want to see actual token lists\n",
    "train_dev_overlap_list = list(train_dev_overlap.elements())\n",
    "train_test_overlap_list = list(train_test_overlap.elements())\n",
    "dev_test_overlap_list = list(dev_test_overlap.elements())\n",
    "all_three_overlap_list = list(all_three_overlap.elements())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "268982f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute intersections to find overlaps of unique tokens\n",
    "train_dev_overlap_set = train_token_set & dev_token_set                # tokens that appear in both train and dev\n",
    "train_test_overlap_set = train_token_set & test_token_set                 # tokens that appear in both train and test\n",
    "dev_test_overlap_set = dev_token_set & test_token_set                     # tokens that appear in both dev and test\n",
    "all_three_overlap_set = train_token_set & dev_token_set & test_token_set     # tokens common to all three splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4e46ec7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Dev overlap: 333 tokens\n",
      "Train-Test overlap: 309 tokens\n",
      "Dev-Test overlap: 105 tokens\n",
      "All three overlap: 100 tokens\n"
     ]
    }
   ],
   "source": [
    "# print out the number of overlapping tokens\n",
    "print(f\"Train-Dev overlap: {len(train_dev_overlap_list)} tokens\")\n",
    "print(f\"Train-Test overlap: {len(train_test_overlap_list)} tokens\")\n",
    "print(f\"Dev-Test overlap: {len(dev_test_overlap_list)} tokens\")\n",
    "print(f\"All three overlap: {len(all_three_overlap_list)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "59751c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Dev overlap (unique): 256 tokens\n",
      "Train-Test overlap (unique): 219 tokens\n",
      "Dev-Test overlap (unique): 78 tokens\n",
      "All three overlap (unique): 74 tokens\n"
     ]
    }
   ],
   "source": [
    "# print out the number of overlapping tokens\n",
    "print(f\"Train-Dev overlap (unique): {len(train_dev_overlap_set)} tokens\")\n",
    "print(f\"Train-Test overlap (unique): {len(train_test_overlap_set)} tokens\")\n",
    "print(f\"Dev-Test overlap (unique): {len(dev_test_overlap_set)} tokens\")\n",
    "print(f\"All three overlap (unique): {len(all_three_overlap_set)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a0476f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create set of the overlapping tokens(unique)\n",
    "overlapping_tokens = set.union(train_dev_overlap_set, train_test_overlap_set, dev_test_overlap_set)\n",
    "\n",
    "len(overlapping_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8bb275f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for removing sentences with overlapping tokens from train, dev, test\n",
    "# and adding them to a new dataset \"overlap_data\"\n",
    "\n",
    "def remove_overlapping_sentences(dataset, overlapping_tokens):\n",
    "    '''\n",
    "    Removes sentences from the dataset that contain any token present in overlapping_tokens.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (List[dict]): The token-tagged dataset (train/dev/test).\n",
    "        overlapping_tokens (Set[str]): Set of overlapping tokens to filter out.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[dict], List[dict]]:\n",
    "            - cleaned_data: List of sentences without overlapping tokens.\n",
    "            - overlap_data: List of sentences with overlapping tokens.\n",
    "    '''\n",
    "    cleaned_data = []\n",
    "    overlap_data = []\n",
    "\n",
    "    for sentence in dataset:\n",
    "        tokens = sentence[\"tokens\"]\n",
    "        \n",
    "        # check for overlap in the sentence\n",
    "        labeled_tokens = extract_labeled_tokens([sentence])\n",
    "        if any(token in overlapping_tokens for token in labeled_tokens):\n",
    "\n",
    "            overlap_data.append(sentence)  # overlapping sentence\n",
    "        else:\n",
    "            cleaned_data.append(sentence)  # clean sentence\n",
    "\n",
    "    return cleaned_data, overlap_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8f1a6b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train overlap: 957 sentences\n",
      "Dev overlap: 186 sentences\n",
      "Test overlap: 183 sentences\n",
      "Total overlap collected: 1326 sentences\n"
     ]
    }
   ],
   "source": [
    "# Remove overlapping sentences\n",
    "train_data_cleaned, train_overlap = remove_overlapping_sentences(train_data, overlapping_tokens)\n",
    "dev_data_cleaned, dev_overlap = remove_overlapping_sentences(dev_data, overlapping_tokens)\n",
    "test_data_cleaned, test_overlap = remove_overlapping_sentences(test_data, overlapping_tokens)\n",
    "\n",
    "# Combine all overlaps into one set for later processing\n",
    "overlap_data = train_overlap + dev_overlap + test_overlap\n",
    "\n",
    "print(f\"Train overlap: {len(train_overlap)} sentences\")\n",
    "print(f\"Dev overlap: {len(dev_overlap)} sentences\")\n",
    "print(f\"Test overlap: {len(test_overlap)} sentences\")\n",
    "print(f\"Total overlap collected: {len(overlap_data)} sentences\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1c1e92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that there is no overlap in the cleaned data\n",
    "# extract tokens with non-\"O\" labels from each split\n",
    "train_tokens_clean_set = set(extract_labeled_tokens(train_data_cleaned))\n",
    "dev_tokens_clean_set = set(extract_labeled_tokens(dev_data_cleaned))\n",
    "test_tokens_clean_set = set(extract_labeled_tokens(test_data_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5fd464ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute intersections to find overlaps of unique tokens\n",
    "train_dev_overlap_clean_set = train_tokens_clean_set & dev_tokens_clean_set                # tokens that appear in both train and dev\n",
    "train_test_overlap_clean_set = train_tokens_clean_set & test_tokens_clean_set                 # tokens that appear in both train and test\n",
    "dev_test_overlap_clean_set = dev_tokens_clean_set & test_tokens_clean_set                     # tokens that appear in both dev and test\n",
    "all_three_overlap_clean_set = train_tokens_clean_set & dev_tokens_clean_set & test_tokens_clean_set     # tokens common to all three splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a65e4fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Dev overlap in cleaned data (unique): 0 tokens\n",
      "Train-Test overlap in cleaned data (unique): 0 tokens\n",
      "Dev-Test overlap in cleaned data (unique): 0 tokens\n",
      "All three overlap in cleaned data (unique): 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# print out the number of overlapping tokens\n",
    "print(f\"Train-Dev overlap in cleaned data (unique): {len(train_dev_overlap_clean_set)} tokens\")\n",
    "print(f\"Train-Test overlap in cleaned data (unique): {len(train_test_overlap_clean_set)} tokens\")\n",
    "print(f\"Dev-Test overlap in cleaned data (unique): {len(dev_test_overlap_clean_set)} tokens\")\n",
    "print(f\"All three overlap in cleaned data (unique): {len(all_three_overlap_clean_set)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e002efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of cleaned train set:  3426\n",
      "Size of cleaned dev set:  378\n",
      "Size of cleaned test set:  382\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of cleaned train set: \", len(train_data_cleaned))\n",
    "print(\"Size of cleaned dev set: \", len(dev_data_cleaned))\n",
    "print(\"Size of cleaned test set: \", len(test_data_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873a6ab",
   "metadata": {},
   "source": [
    "#### Group overlap data by tokens \n",
    "- So all sentences containing ex. \"Denmark\" are together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a40b9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_overlap_data(overlap_data, split_ratio=(0.2, 0.4, 0.4), seed=42,\n",
    "                       existing_split_tokens=None):\n",
    "    '''\n",
    "    Splits the overlap_data into train/dev/test with no overlapping labeled tokens.\n",
    "    existing_split_tokens: optional dict with 'train', 'dev', 'test' -> sets of labeled tokens already in those splits\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    shuffled = list(enumerate(overlap_data))\n",
    "    random.shuffle(shuffled)\n",
    "\n",
    "    split_data = {'train': [], 'dev': [], 'test': []}\n",
    "    split_tokens = {'train': set(), 'dev': set(), 'test': set()}\n",
    "\n",
    "    # Initialize existing token sets if provided\n",
    "    if existing_split_tokens is None:\n",
    "        existing_split_tokens = {'train': set(), 'dev': set(), 'test': set()}\n",
    "\n",
    "    def can_add(tokens, split_name):\n",
    "        others = [s for s in split_tokens if s != split_name]\n",
    "\n",
    "        # Check against both the other split_tokens and the existing_split_tokens\n",
    "        for other in others:\n",
    "            for token in tokens:\n",
    "                if token in split_tokens[other] or token in existing_split_tokens[other]:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    unassigned = []\n",
    "\n",
    "    for idx, sentence in shuffled:\n",
    "        tokens = set(extract_labeled_tokens([sentence]))\n",
    "\n",
    "        for split in ['train', 'dev', 'test']:\n",
    "            if can_add(tokens, split):\n",
    "                split_data[split].append(sentence)\n",
    "                split_tokens[split].update(tokens)\n",
    "                break\n",
    "        else:\n",
    "            unassigned.append(sentence)  # none of the splits could take it\n",
    "\n",
    "    return split_data['train'], split_data['dev'], split_data['test'], unassigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0c096b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_split_tokens = {\n",
    "    'train': set(extract_labeled_tokens(train_data_cleaned)),\n",
    "    'dev': set(extract_labeled_tokens(dev_data_cleaned)),\n",
    "    'test': set(extract_labeled_tokens(test_data_cleaned)),\n",
    "}\n",
    "\n",
    "train_overlap_clean, dev_overlap_clean, test_overlap_clean, unassigned = split_overlap_data(\n",
    "    overlap_data,\n",
    "    existing_split_tokens=existing_split_tokens\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "91edd755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap_train_dev: 0\n",
      "overlap_train_test: 0\n",
      "overlap_dev_test: 0\n",
      "Final train size: 4708\n",
      "Final dev size: 380\n",
      "Final test size: 394\n",
      "total:  5482\n",
      "Unassigned sentences due to overlap constraints: 30\n",
      "['Serbiens', 'Slobodan', 'Milosevic', 'USA', 'Rusland', 'Bosnien-Hercegovina', 'Serbiens', 'DSB', 'Ask', 'Urd', 'Henning', 'Camre', 'Jørgen', 'Leth', 'Det', 'perfekte', 'menneske', 'Det', 'gode', 'og', 'det', 'onde', 'Notater', 'om', 'kærligheden', 'Simon', 'Wiesenthal', 'Poul', 'Schlüter', 'IV', '.', 'Poul', 'Schlüters', 'Folketinget', 'Kamilla', 'Rekefjord', 'Norge', 'Rendsburg', 'Tyskland', 'Kirsten', 'Jacobsen', 'Landsforeningen', 'Ungbo', 'Ungbos', 'Ungbos', 'Torben', 'Lund', 'TV-2-Nyhederne', 'Ungbos', 'Urd', 'Århus', 'DSB', 'Danmarks', 'EFs', 'Peter', 'Duetoft', 'Keld', 'Krogh', 'Brian', 'Lentz', 'CBC', 'Subaru', 'Schweiz', 'Tyskland', 'Poul', 'Schlüter', 'Folketinget', 'Sverige', 'Danmark', 'Tyskland', 'DSBs', 'Urd', 'Århus', 'Havns', 'Hermes', 'Lone', 'Scherfig', 'Æterdrama', 'Hvor', 'er', 'det', 'dog', 'synd', 'for', 'Dora', 'Danmark', 'Tyskland', 'Lars', 'Bred', 'NUGA', 'Tyskland', 'Sverige', 'det', 'svenske', 'filminstitut', 'Tyskland', 'Storbritannien', 'Frankrig', 'Grækenland', 'Bonn', 'D-marken', 'Tyskland', 'EF', 'Ombudsmanden', 'Folketingets', 'VS', 'SF', 'Folketinget', 'Pedersen', '.', 'Ask', 'Ole', 'Rønnow', 'Peder', 'Paars', 'Jan', 'Køpke', 'Christensen', 'Folketinget', 'Schlüter', 'Aage', 'Brusgaard', 'Dyremose', 'Peter', 'Duetoft', 'Jens', 'Peter', 'Bonde', 'Folkebevægelsen', 'mod', 'EF', 'Europa', 'EF', 'Tyskland', 'Danmark', 'Tyskland']\n",
      "136\n",
      "2917\n",
      "74\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "final_train = train_data_cleaned + train_overlap_clean\n",
    "final_dev = dev_data_cleaned + dev_overlap_clean\n",
    "final_test = test_data_cleaned + test_overlap_clean\n",
    "\n",
    "# Sanity check for labeled token overlaps\n",
    "train_tokens = set(extract_labeled_tokens(final_train))\n",
    "dev_tokens = set(extract_labeled_tokens(final_dev))\n",
    "test_tokens = set(extract_labeled_tokens(final_test))\n",
    "\n",
    "print(f\"overlap_train_dev: {len(train_tokens & dev_tokens)}\")\n",
    "print(f\"overlap_train_test: {len(train_tokens & test_tokens)}\")\n",
    "print(f\"overlap_dev_test: {len(dev_tokens & test_tokens)}\")\n",
    "\n",
    "print(f\"Final train size: {len(final_train)}\")\n",
    "print(f\"Final dev size: {len(final_dev)}\")\n",
    "print(f\"Final test size: {len(final_test)}\")\n",
    "print(\"total: \", len(final_train) + len(final_dev) + len(final_test))\n",
    "\n",
    "print(f\"Unassigned sentences due to overlap constraints: {len(unassigned)}\")\n",
    "print(extract_labeled_tokens(unassigned))\n",
    "print(len(extract_labeled_tokens(unassigned)))\n",
    "\n",
    "print(len(train_tokens))\n",
    "print(len(dev_tokens))\n",
    "print(len(test_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d4f2a",
   "metadata": {},
   "source": [
    "## Checking for overlap in new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e44b88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.load_data import extract_labeled_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4767fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"../data/da_news_new/new_da_news_train.tsv\"\n",
    "path_dev = \"../data/da_news_new/new_da_news_dev.tsv\"\n",
    "path_test = \"../data/da_news_new/new_da_news_test.tsv\"\n",
    "\n",
    "label2id, id2label = mapping(path_train)\n",
    "\n",
    "train_data = read_tsv_file(path_train, label2id)\n",
    "dev_data = read_tsv_file(path_dev, label2id)\n",
    "test_data = read_tsv_file(path_test, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "22e390cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 1745\n",
      "dev size: 161\n",
      "test size: 344\n"
     ]
    }
   ],
   "source": [
    "print(\"train size:\", len(train_data))\n",
    "print(\"dev size:\", len(dev_data))\n",
    "print(\"test size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "14460d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap_train_dev: 141\n",
      "overlap_train_test: 188\n",
      "overlap_dev_test: 54\n"
     ]
    }
   ],
   "source": [
    "train_tokens = extract_labeled_tokens(train_data)\n",
    "dev_tokens = extract_labeled_tokens(dev_data)\n",
    "test_tokens = extract_labeled_tokens(test_data)\n",
    "\n",
    "print(f\"overlap_train_dev: {len(train_tokens & dev_tokens)}\")\n",
    "print(f\"overlap_train_test: {len(train_tokens & test_tokens)}\")\n",
    "print(f\"overlap_dev_test: {len(dev_tokens & test_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "97904e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens & dev_tokens & test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a00f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
